{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data 101 with `gluon` and `gluonnlp` - the second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:41:24.120166Z",
     "start_time": "2018-08-20T21:41:24.115671Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This notebook continues our demonstration of GluonNLP's tools for getting data into the shape required for training. In particular, we cover\n",
    "\n",
    "- Transforming data to batches\n",
    "- DataStream abstractions in `gluonnlp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:35.222621Z",
     "start_time": "2018-08-20T21:45:33.757080Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating batches - Batchify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To train our models, data alone is not sufficient.\n",
    "It must both be in the right format for the task and sufficiently arranged for efficient computation.\n",
    "\n",
    "This usually means that we need to group data together in batches so that we can proess multiple examples in parallel, getting the best use out of the available GPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For efficient training we need to transform the input data into batches.\n",
    "Consider a model that takes independent sentences as inputs, to batchify the data,\n",
    "each sentence in a batch must have the same length.\n",
    "As it is unlikely to find sufficient sentences of every possible length\n",
    "it is common to pad all sentences in a batch to the same length.\n",
    "\n",
    "Such padded inputs are common in machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:36.023985Z",
     "start_time": "2018-08-20T21:45:35.224563Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = nlp.data.WikiText2(flatten=False)\n",
    "vocab = nlp.Vocab(nlp.data.count_tokens(itertools.chain.from_iterable(data)))\n",
    "coded = data.transform(vocab.to_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For WikiText2, the first 100 paragraphs have the following lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:36.028771Z",
     "start_time": "2018-08-20T21:45:36.025844Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 128, 92, 102, 6, 212, 297, 85, 6, 189, 222, 204, 6, 268, 249, 8, 305, 8, 267, 90, 6, 66, 125, 145, 93, 6, 65, 8, 196, 223, 120, 10, 100, 91, 6, 210, 7, 283, 91, 10, 179, 144, 45, 17, 22, 22, 76, 87, 58, 10, 10, 9, 8, 5, 10, 4, 5, 10, 5, 176, 86, 59, 195, 176, 117, 130, 80, 72, 48, 21, 10, 10, 4, 9, 9, 5, 3, 15, 11, 5, 17, 5, 4, 3, 9, 91, 118, 35, 21, 6, 122, 160, 7, 109, 56, 7, 185, 43, 118, 110]\n"
     ]
    }
   ],
   "source": [
    "print([len(s) for s in data[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"no_bucket_strategy.png\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `gluonnlp.data.batchify.Pad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`gluonnlp.data.batchify.Pad` can be instantiated with the padding value.\n",
    "The resulting function takes a list of variable length lists (or NDArrays or numpy arrays)\n",
    "as input and returns a single NDArray where all shorter inputs are padded to the length\n",
    "of the maximum length input element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:36.033928Z",
     "start_time": "2018-08-20T21:45:36.030462Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 128\n"
     ]
    }
   ],
   "source": [
    "print(len(coded[0]), len(coded[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:36.038660Z",
     "start_time": "2018-08-20T21:45:36.035590Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_val: 1\n"
     ]
    }
   ],
   "source": [
    "pad_val = vocab[vocab.padding_token]\n",
    "pad = nlp.data.batchify.Pad(pad_val=pad_val)\n",
    "print('pad_val:', pad_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:36.054702Z",
     "start_time": "2018-08-20T21:45:36.040301Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (2, 128)\n",
      "\n",
      "[[   12  3933  4430   853    12     3     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1]\n",
      " [21730   129  3933    92    45     0  4430    25   754    45 33278     5\n",
      "   7184     6  3933     7     4 23715    92    24     5  1912   988    10]]\n",
      "<NDArray 2x24 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('Shape', pad([coded[0], coded[1]]).shape)\n",
    "print(pad([coded[0], coded[1]]).astype(int)[:, :24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `gluon.data.DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Manually sampling sentences from the dataset and applying the pad function may be bothersome.\n",
    "`gluon.data.DataLoader` automates this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:36.058929Z",
     "start_time": "2018-08-20T21:45:36.056378Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads data from a dataset and returns mini-batches of data.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    dataset : Dataset\n",
      "        Source dataset. Note that numpy and mxnet arrays can be directly used\n",
      "        as a Dataset.\n",
      "    batch_size : int\n",
      "        Size of mini-batch.\n",
      "    shuffle : bool\n",
      "        Whether to shuffle the samples.\n",
      "    sampler : Sampler\n",
      "        The sampler to use. Either specify sampler or shuffle, not both.\n",
      "    last_batch : {'keep', 'discard', 'rollover'}\n",
      "        How to handle the last batch if batch_size does not evenly divide\n",
      "        `len(dataset)`.\n",
      "\n",
      "        keep - A batch with less samples than previous batches is returned.\n",
      "        discard - The last batch is discarded if its incomplete.\n",
      "        rollover - The remaining samples are rolled over to the next epoch.\n",
      "    batch_sampler : Sampler\n",
      "        A sampler that returns mini-batches. Do not specify batch_size,\n",
      "        shuffle, sampler, and last_batch if batch_sampler is specified.\n",
      "    batchify_fn : callable\n",
      "        Callback function to allow users to specify how to merge samples\n",
      "        into a batch. Defaults to `default_batchify_fn`::\n",
      "\n",
      "            def default_batchify_fn(data):\n",
      "                if isinstance(data[0], nd.NDArray):\n",
      "                    return nd.stack(*data)\n",
      "                elif isinstance(data[0], tuple):\n",
      "                    data = zip(*data)\n",
      "                    return [default_batchify_fn(i) for i in data]\n",
      "                else:\n",
      "                    data = np.asarray(data)\n",
      "                    return nd.array(data, dtype=data.dtype)\n",
      "\n",
      "    num_workers : int, default 0\n",
      "        The number of multiprocessing workers to use for data preprocessing.\n",
      "    pin_memory : boolean, default False\n",
      "        If ``True``, the dataloader will copy NDArrays into pinned memory\n",
      "        before returning them. Copying from CPU pinned memory to GPU is faster\n",
      "        than from normal CPU memory.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(gluon.data.DataLoader.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:38.430458Z",
     "start_time": "2018-08-20T21:45:36.060647Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of batches is 192.7940087512622\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "data_loader = gluon.data.DataLoader(coded, batchify_fn=pad, batch_size=batch_size)\n",
    "print('Average length of batches is', sum(batch.shape[1] for batch in data_loader) / len(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T20:37:20.742611Z",
     "start_time": "2018-08-20T20:37:20.738625Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"fixed_bucket_strategy_ratio0.7.png\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Custom `Sampler` for `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sampling random sentences from the Dataset and padding them is sub-optimal as the number of padding elments is determined by the longest sentence.\n",
    "`DataLoader` supports the specification of a `Sampler` to specify the sentences to select for a batch.\n",
    "\n",
    "For example, `gluonnlp.data.FixedBucketSampler` assigns each data sample (sentence)\n",
    "to a fixed bucket based on its length. Resulting batches will only contain sentences\n",
    "from a single bucket.\n",
    "\n",
    "This can significantly reduce the average number of elements per batch and consequently the amount of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:39.493163Z",
     "start_time": "2018-08-20T21:45:38.432297Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sampler = nlp.data.FixedBucketSampler(lengths=[len(c) for c in coded], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:39.497514Z",
     "start_time": "2018-08-20T21:45:39.494994Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_loader = gluon.data.DataLoader(coded, batchify_fn=pad, batch_sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:41.835361Z",
     "start_time": "2018-08-20T21:45:39.499108Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of batches is 117.92067226890757\n"
     ]
    }
   ],
   "source": [
    "print('Average length of batches is', sum(batch.shape[1] for batch in data_loader) / len(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `DataStream` for large corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "``` python\n",
    "class DataStream(object):\n",
    "    # Iterate over stream\n",
    "    # `for sample in stream: print(sample)`\n",
    "    def __iter__(self):\n",
    "        ...\n",
    "\n",
    "    def transform(self, fn):\n",
    "        # Returns a new DataStream\n",
    "        # Each sample transformed by the function `fn`\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Corpora for various NLP tasks can be very large - too large to load them at once to memory.\n",
    "Luckily there is seldomly a need to load the corpus at once. Instead we can stream the necessary chunks of data that we need at any point in time off the disk, dealing only with the batches that we need for the current training iterations.\n",
    "In `gluonnlp`, the `DataStream` concept together with other utilities makes such data pipelines easy. \n",
    "\n",
    "Any Python iterable can be wrapped in a `DataStream` using `gluonnlp.data.SimpleDataStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:41.841513Z",
     "start_time": "2018-08-20T21:45:41.837230Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "data = nlp.data.SimpleDataStream(\n",
    "    [\n",
    "        [1,2,3],\n",
    "        [4,5,6]\n",
    "    ]\n",
    ")\n",
    "transformed = data.transform(lambda x: [e + 1 for e in x])\n",
    "\n",
    "for sample in transformed:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example - Google Billion Words Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As GBW is very large, `gluonnlp` contains a convenient `DataStream` representation of the corpus.\n",
    "The corpus was split in 100 parts of around ~40MB size (containing ~300k sentences each).\n",
    "The Stream iterates over `CorpusDataset` - one for each of the 100 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:47.968254Z",
     "start_time": "2018-08-20T21:45:41.843214Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gbw = nlp.data.GBWStream(segment='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:52.110215Z",
     "start_time": "2018-08-20T21:45:47.970263Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gluonnlp.data.dataset.CorpusDataset object at 0x7fb5dd452f60>\n",
      "<gluonnlp.data.dataset.CorpusDataset object at 0x7fb5dd4529b0>\n"
     ]
    }
   ],
   "source": [
    "for gbw_element in itertools.islice(gbw, 2):\n",
    "    print(gbw_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:52.115544Z",
     "start_time": "2018-08-20T21:45:52.112072Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'well', 'as', 'funding', 'equipment', ',', 'the', 'foundation', 'has', 'paid']\n",
      "['The', 'alert', 'among', 'you', 'will', 'have', 'noticed', 'by', 'now', 'that']\n",
      "['Speaking', 'at', 'a', 'rally', 'at', 'a', 'Las', 'Vegas', 'casino', ',']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(gbw_element[i][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `DatasetStream`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A `DataStream` that iterates over `Dataset`s is also called a `DatasetStream`.\n",
    "`gluonnlp` contains the `SimpleDatasetStream`,\n",
    "which allows you to easily construct a Stream similar to `GBWStream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:52.119916Z",
     "start_time": "2018-08-20T21:45:52.117206Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A simple stream of Datasets.\n",
      "\n",
      "    The SimpleDatasetStream is created from multiple files based on provided\n",
      "    `file_pattern`. One file is read at a time and a corresponding Dataset is\n",
      "    returned. The Dataset is created based on the file and the kwargs passed to\n",
      "    SimpleDatasetStream.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    dataset : class\n",
      "        The class for which to create an object for every file. kwargs are\n",
      "        passed to this class.\n",
      "    file_pattern: str\n",
      "        Path to the input text files.\n",
      "    file_sampler : str, {'sequential', 'random'}, defaults to 'random'\n",
      "        The sampler used to sample a file.\n",
      "\n",
      "        - 'sequential': SequentialSampler\n",
      "        - 'random': RandomSampler\n",
      "    kwargs\n",
      "        All other keyword arguments are passed to the dataset constructor.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(nlp.data.SimpleDatasetStream.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In short, a language model tries to predict the next word given the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"Recurrent_neural_network_unfold.svg\" style=\"width: 66%;\"/>\n",
    "Image from https://en.wikipedia.org/wiki/Recurrent_neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Back-propagation-through-time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For language models no padding is required and batches are instead packed with samples as tight as possible.\n",
    "There is neither any guarantee, nor requirement that a sentence fits in a single batch.\n",
    "Instead subsequent batches must be chosen such that they contain valid continuations of the previous sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We train the model using truncated back-propagation-through-time (BPTT).\n",
    "We backpropagate for 35 time steps using stochastic gradient descent.\n",
    "For efficient optimizations, we need to obtain a batch of source and target arrays.\n",
    "\n",
    "For truncated BPTT we usually do not reset the hidden states of our RNN,\n",
    "making it important that batches following upon each other actually are\n",
    "valid continuations of the previous batch.\n",
    "\n",
    "For a batch size of 1, the first two data batches would contain the following words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:52.807672Z",
     "start_time": "2018-08-20T21:45:52.121590Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batchify = nlp.data.batchify.StreamBPTTBatchify(gbw.vocab, seq_len=5, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:52.811984Z",
     "start_time": "2018-08-20T21:45:52.809501Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bptt_batches = batchify(gbw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:55.125671Z",
     "start_time": "2018-08-20T21:45:52.813608Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data \n",
      "[[   16   373]\n",
      " [35574    11]\n",
      " [  465  7484]\n",
      " [ 1427  5305]\n",
      " [ 1897  1129]]\n",
      "<NDArray 5x2 @cpu(0)>\n",
      "target \n",
      "[[35574    11]\n",
      " [  465  7484]\n",
      " [ 1427  5305]\n",
      " [ 1897  1129]\n",
      " [  713   122]]\n",
      "<NDArray 5x2 @cpu(0)> \n",
      "\n",
      "data \n",
      "[[  713   122]\n",
      " [39131    18]\n",
      " [    7     8]\n",
      " [10503 42884]\n",
      " [   64  4548]]\n",
      "<NDArray 5x2 @cpu(0)>\n",
      "target \n",
      "[[39131    18]\n",
      " [    7     8]\n",
      " [10503 42884]\n",
      " [   64  4548]\n",
      " [   27    21]]\n",
      "<NDArray 5x2 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in itertools.islice(bptt_batches, 2):\n",
    "    print('data', batch[0].astype(int))\n",
    "    print('target', batch[1].astype(int), '\\n')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
