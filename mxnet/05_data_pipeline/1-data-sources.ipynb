{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data 101 with `gluon` and `gluonnlp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In order to train neural networks, we need data.\n",
    "GluonNLP provides useful abstractions for getting textual data in the shape required to train typical deep nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this notebook, we will cover the following:\n",
    "- Dataset abstractions in `gluon`\n",
    "- Using included Datasets as well as custom Datasets\n",
    "- Common data transformations in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:07.211610Z",
     "start_time": "2018-08-20T22:16:05.800165Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset in `gluon`\n",
    "\n",
    "Datasets in Gluon have the following basic structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "``` python\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, idx):\n",
    "        ...\n",
    "    \n",
    "    def __len__(self):\n",
    "        ...\n",
    "\n",
    "    def transform(self, fn, lazy=True):\n",
    "        # Returns a new dataset with each sample\n",
    "        # transformed by the function `fn`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can make list-like Python object (ie. that implements `__getitem__` meaning it  subscripted like `x[0]` etc.),\n",
    "into a `gluon` `Dataset` by wrapping it, using `gluon.data.SimpleDataset` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:07.216316Z",
     "start_time": "2018-08-20T22:16:07.213552Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "simple_data = gluon.data.SimpleDataset([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:07.220741Z",
     "start_time": "2018-08-20T22:16:07.218028Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "transformed = simple_data.transform(lambda x: [e + 1 for e in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:07.225509Z",
     "start_time": "2018-08-20T22:16:07.222419Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n",
      "[5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "print(transformed[0])\n",
    "print(transformed[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Provided datasets\n",
    "\n",
    "- GluonNLP features a number of popular benchmark datasets out of the box in order to make it easy for you to get going.\n",
    "  - http://gluon-nlp.mxnet.io/api/data.html\n",
    "  \n",
    "Using these datasets just requires instantiating a class. Here we take WikiText2 as example. This datasrt is distributed as text with one paragraph per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:07.530451Z",
     "start_time": "2018-08-20T22:16:07.227238Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_w2 = nlp.data.WikiText2(segment='train', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:07.535050Z",
     "start_time": "2018-08-20T22:16:07.532262Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiText2 with flatten=False contains 23767 samples\n"
     ]
    }
   ],
   "source": [
    "print('WikiText2 with flatten=False contains', len(data_w2), 'samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:07.539994Z",
     "start_time": "2018-08-20T22:16:07.536715Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']\n",
      "['Senjō', 'no', 'Valkyria', '3', ':', '<unk>', 'Chronicles', '(', 'Japanese', ':']\n",
      "['The', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(data_w2[0][:10])\n",
    "print(data_w2[1][:10])\n",
    "print(data_w2[2][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Each sample in `data` is a list of words, representing a paragraph in the WikiText2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1 - flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some tasks don't operate on paragraphs and thus ignore paragraph boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:08.048888Z",
     "start_time": "2018-08-20T22:16:07.541665Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_w2_f = nlp.data.WikiText2(segment='train', flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:08.053515Z",
     "start_time": "2018-08-20T22:16:08.050734Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiText2 with flatten=True contains 2075677 tokens\n"
     ]
    }
   ],
   "source": [
    "print('WikiText2 with flatten=True contains', len(data_w2_f), 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:08.057980Z",
     "start_time": "2018-08-20T22:16:08.055157Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 words are:  ['=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>', 'Senjō', 'no', 'Valkyria', '3']\n"
     ]
    }
   ],
   "source": [
    "print('The first 10 words are: ', data_w2_f[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example 2 - BOS / EOS\n",
    "\n",
    "Some tasks denote the beginning and end of a sentence with special BOS (beginning-of-sentence) and EOS (end-of-sentence) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:08.456023Z",
     "start_time": "2018-08-20T22:16:08.059657Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiText2 with flatten=True and eos contains 2075677 tokens\n",
      "The first 10 words are:  ['=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>', 'Senjō', 'no', 'Valkyria', '3']\n"
     ]
    }
   ],
   "source": [
    "data_w2_eos = nlp.data.WikiText2(segment='train', flatten=True, bos=None, eos='<eos>')\n",
    "print('WikiText2 with flatten=True and eos contains', len(data_w2_eos), 'tokens')\n",
    "print('The first 10 words are: ', data_w2_eos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `WikiText2` behind the scenes: `CorpusDataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- `gluonnlp` provides `CorpusDataset` which makes it easy to read custom corpora based on provided sample splitter and word tokenizer\n",
    "- Let's use `CorpusDataset` to manually read WikiText2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, \"standard\" Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:08.489544Z",
     "start_time": "2018-08-20T22:16:08.457794Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('wiki.train.tokens', encoding='utf-8') as f:\n",
    "    raw_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:08.494508Z",
     "start_time": "2018-08-20T22:16:08.491420Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStatio\n"
     ]
    }
   ],
   "source": [
    "print(raw_data[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's look at the GluonNLP functionality to read a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:08.772452Z",
     "start_time": "2018-08-20T22:16:08.496132Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_w2_raw = nlp.data.CorpusDataset(\n",
    "    'wiki.train.tokens',\n",
    "    encoding='utf8',\n",
    "    flatten=False,\n",
    "    skip_empty=True,\n",
    "    sample_splitter=lambda x: x.splitlines(),\n",
    "    tokenizer=lambda x: x.split(),\n",
    "    bos=None,\n",
    "    eos=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:08.777777Z",
     "start_time": "2018-08-20T22:16:08.774327Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', 'Valkyria', 'Chronicles', 'III', '=']\n",
      "['Senjō', 'no', 'Valkyria', '3', ':', '<unk>', 'Chronicles', '(', 'Japanese', ':', '戦場のヴァルキュリア3', ',']\n",
      "['The', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(data_w2_raw[i][:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vocabulary and coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Vocabulay provides one-to-one mapping between tokens and integer indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:09.060630Z",
     "start_time": "2018-08-20T22:16:08.779456Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable(data_w2_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:09.132389Z",
     "start_time": "2018-08-20T22:16:09.062460Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=33280, unk=\"<unk>\", reserved=\"['<pad>', '<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "vocab = nlp.Vocab(counter=counter,\n",
    "          max_size=None,\n",
    "          min_freq=1,\n",
    "          unknown_token='<unk>',\n",
    "          padding_token='<pad>',\n",
    "          bos_token='<bos>',\n",
    "          eos_token='<eos>',\n",
    "          reserved_tokens=None)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The vocabulary object enables us to replace a token or list of tokens `t`\n",
    "with the corresponding indices via\n",
    "`vocab[t]` or equivalently `vocab.to_indices(t)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:09.136993Z",
     "start_time": "2018-08-20T22:16:09.134026Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', 'Valkyria', 'Chronicles', 'III', '=']       becomes      [12, 3933, 4430, 853, 12]\n",
      "['Senjō', 'no', 'Valkyria', '3', ':']             becomes      [21730, 129, 3933, 92, 45]\n",
      "['The', 'game', 'began', 'development', 'in']     becomes      [15, 79, 135, 443, 9]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    data_i = data_w2_raw[i][:5]\n",
    "    print('{:<50}{:<13}{}'.format(str(data_i), 'becomes', str(vocab[data_i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can use the Dataset `transform` API to apply the `to_indices` method of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:09.141868Z",
     "start_time": "2018-08-20T22:16:09.138631Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 3933, 4430, 853, 12]\n",
      "[21730, 129, 3933, 92, 45, 0, 4430, 25, 754, 45]\n",
      "[15, 79, 135, 443, 9, 283, 5, 3332, 73, 11]\n"
     ]
    }
   ],
   "source": [
    "coded = data_w2_raw.transform(vocab.to_indices)\n",
    "\n",
    "for i in range(3):\n",
    "    print(coded[i][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practice - Wikitext-2-Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "WikiText-2 is distributed in two versions.\n",
    "- Standard version\n",
    "  - Preprocessed by replacing infrequent tokens with `<unk>``\n",
    "- Raw version\n",
    "\n",
    "Let's use `gluonnlp` arrive at the pre-processed version given the raw version.\n",
    "\n",
    "A few tips:\n",
    "- GluonNLP provides nlp.data.WikiText2Raw to load the raw version.\n",
    "- nlp.data.WikiText2Raw default parameters are for character level language modeling\n",
    "  - To reconstruct the pre-processed WikiText2, a different `tokenizer` argument needs to be passed\n",
    "- When constructing the vocabulary, you must specify a maximum size based on the number of words in WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:09.146166Z",
     "start_time": "2018-08-20T22:16:09.143521Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiText-2 character-level dataset for language modeling\n",
      "\n",
      "    WikiText2Raw is implemented as CorpusDataset with the default flatten=True.\n",
      "\n",
      "    From Salesforce research:\n",
      "    https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset\n",
      "\n",
      "    License: Creative Commons Attribution-ShareAlike\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    segment : {'train', 'val', 'test'}, default 'train'\n",
      "        Dataset segment.\n",
      "    flatten : bool, default True\n",
      "        Whether to return all samples as flattened tokens. If True, each sample is a token.\n",
      "    skip_empty : bool, default True\n",
      "        Whether to skip the empty samples produced from sample_splitters. If False, `bos` and `eos`\n",
      "        will be added in empty samples.\n",
      "    tokenizer : function, default s.encode('utf-8')\n",
      "        A function that splits each sample string into list of tokens.\n",
      "        The tokenizer can also be used to convert everything to lowercase.\n",
      "        E.g. with tokenizer=lambda s: s.lower().encode('utf-8')\n",
      "    bos : str or None, default None\n",
      "        The token to add at the begining of each sentence. If None, nothing is added.\n",
      "    eos : str or None, default '<eos>'\n",
      "        The token to add at the end of each sentence. If None, nothing is added.\n",
      "    root : str, default '$MXNET_HOME/datasets/wikitext-2'\n",
      "        Path to temp folder for storing data.\n",
      "        MXNET_HOME defaults to '~/.mxnet'.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(nlp.data.WikiText2Raw.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:09.150802Z",
     "start_time": "2018-08-20T22:16:09.147816Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# data = nlp.data.WikiText2Raw(...)\n",
    "# vocab = \n",
    "# coded = data.transform(vocab.to_indices)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
