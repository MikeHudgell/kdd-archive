{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Pre-trained Word Embeddings\n",
    "\n",
    "In this notebook, we'll demonstrate how to use pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To see why word embeddings are useful, it's worth comparing them to the alternative.\n",
    "Without word embeddings, we might represent each word by a one-hot vector `[0, ...,0, 1, 0, ... 0]`,\n",
    "that takes value `1` at the index corresponding to the appropriate vocabulary word, \n",
    "and value `0` everywhere else. \n",
    "The weight matrices connecting our word-level inputs to the network's hidden layers would each be $v \\times h$,\n",
    "where $v$ is the size of the vocabulary and $h$ is the size of the hidden layer. \n",
    "With 100,000 words feeding into an LSTM layer with $1000$ nodes, we would need to learn\n",
    "$4$ different weight matrices (one for each of the LSTM gates), each with 100M weights, and thus 400 million parameters in total.\n",
    "\n",
    "Fortunately, it turns out that a number of efficient techniques \n",
    "can quickly discover broadly useful word embeddings in an *unsupervised* manner.\n",
    "These embeddings map each word onto a low-dimensional vector $w \\in R^d$ with $d$ commonly chosen to be roughly $100$.\n",
    "Intuitively, these embeddings are chosen based on the contexts in which words appear. \n",
    "Words that appear in similar contexts (like \"tennis\" and \"racquet\") should have similar embeddings\n",
    "while words that do not like (like \"rat\" and \"gourmet\") should have dissimilar embeddings.\n",
    "\n",
    "Practitioners of deep learning for NLP typically inititalize their models \n",
    "using *pretrained* word embeddings, bringing in outside information, and reducing the number of parameters that a neural network needs to learn from scratch.\n",
    "\n",
    "\n",
    "Two popular word embeddings are GloVe and fastText. \n",
    "The following examples uses pre-trained word embeddings drawn from the following sources:\n",
    "\n",
    "* GloVe project website：https://nlp.stanford.edu/projects/glove/\n",
    "* fastText project website：https://fasttext.cc/\n",
    "\n",
    "To begin, let's first import a few packages that we'll need for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from mxnet import nd\n",
    "import gluonnlp as nlp\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating Vocabulary with Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "Now we'll demonstrate how to index words, \n",
    "attach pre-trained word embeddings for them, \n",
    "and use such embeddings in Gluon. \n",
    "First, let's assign a unique ID and word vector to each word in the vocabulary\n",
    "in just a few lines of code.\n",
    "\n",
    "### Creating Vocabulary from Data Sets\n",
    "\n",
    "To begin, suppose that we have a simple text data set consisting of newline-separated strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = \" hello world \\n hello nice world \\n hi world \\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To start, let's implement a simple tokenizer to separate the words and then count the frequency of each word in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def simple_tokenize(source_str, token_delim=' ', seq_delim='\\n'):\n",
    "    return filter(None, re.split(token_delim + '|' + seq_delim, source_str))\n",
    "\n",
    "counter = nlp.data.count_tokens(simple_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The obtained `counter` behaves like a Python dictionary whose key-value pairs consist of words and their frequencies, respectively. \n",
    "We can then instantiate a `Vocab` object with a counter. \n",
    "Because `counter` tracks word frequencies, we are able to specify arguments \n",
    "such as `max_size` and `min_freq` to the `Vocab` constructor for restricting the size of the resulting vocabulary. Suppose that we want to build indices for all the keys in counter. \n",
    "If we simply want to construct a  `Vocab` containing every word, then we can supply `counter`  the only argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A `Vocab` object associates each word with an index. We can easily access words by their indices using the `vocab.idx_to_token` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>\n",
      "<pad>\n",
      "<bos>\n",
      "<eos>\n",
      "world\n",
      "hello\n",
      "hi\n",
      "nice\n"
     ]
    }
   ],
   "source": [
    "for word in vocab.idx_to_token:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the opposite direction, we can grab an idex given a token using `vocab.token_to_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(vocab.token_to_idx[\"<unk>\"])\n",
    "print(vocab.token_to_idx[\"world\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attaching word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our next step will be to attach word embeddings to the words indexed by `vocab`.\n",
    "In this example, we'll use *fastText* embeddings trained on the *wiki.simple* dataset.\n",
    "First, we'll want to create a word embedding instance by calling `nlp.embedding.create`,\n",
    "specifying the embedding type (un-named argument) `fasttext` and the named argument `source='wiki.simple'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file wiki.simple.npz is not found. Downloading from Gluon Repository. This may take some time.\n",
      "Downloading /home/ubuntu/.mxnet/embedding/fasttext/wiki.simple.npz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/fasttext/wiki.simple.npz...\n"
     ]
    }
   ],
   "source": [
    "fasttext_simple = nlp.embedding.create('fasttext', source='wiki.simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To attach the newly loaded word embeddings `fasttext_simple` to indexed words in `vocab`, we simply call vocab's `set_embedding` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vocab.set_embedding(fasttext_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To see other available sources of pretrained word embeddings using the *fastText* algorithm, \n",
    "we can call `text.embedding.list_sources`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crawl-300d-2M', 'wiki.aa', 'wiki.ab', 'wiki.ace', 'wiki.ady']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.embedding.list_sources('fasttext')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The created vocabulary `vocab` includes four different words and a special\n",
    "unknown token. Let us check the size of `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has length 8\n",
      "['<unk>', '<pad>', '<bos>', '<eos>', 'world', 'hello', 'hi', 'nice']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary has length', len(vocab))\n",
    "print(vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "By default, the vector of any token that is unknown to `vocab` is a zero vector.\n",
    "Its length is equal to the vector dimension of the fastText word embeddings:\n",
    "300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['beautiful'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The first five elements of the vector of any unknown token are zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0. 0. 0. 0. 0.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['beautiful'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Known words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us check the shape of the embedding of words 'hello' and 'world' from\n",
    "`vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['hello', 'world'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can access the first five elements of the embedding of 'hello' and 'world'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.39567   0.21454  -0.035389 -0.24299  -0.095645]\n",
       " [ 0.10444  -0.10858   0.27212   0.13299  -0.33165 ]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.embedding['hello', 'world'][:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using Pre-trained Word Embeddings in Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To demonstrate how to use pre-\n",
    "trained word embeddings in Gluon, let us first obtain indices of the words\n",
    "'hello' and 'world'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['hello', 'world']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can obtain the vectors for the words 'hello' and 'world' by specifying their\n",
    "indices (2 and 1) and the weight matrix `vocab.embedding.idx_to_vec` in\n",
    "`gluon.nn.Embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_dim, output_dim = vocab.embedding.idx_to_vec.shape\n",
    "layer = gluon.nn.Embedding(input_dim, output_dim)\n",
    "layer.initialize()\n",
    "layer.weight.set_data(vocab.embedding.idx_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.39567   0.21454  -0.035389 -0.24299  -0.095645]\n",
       " [ 0.10444  -0.10858   0.27212   0.13299  -0.33165 ]]\n",
       "<NDArray 2x5 @cpu(0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(nd.array([5, 4]))[:, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating Vocabulary from Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can also create\n",
    "vocabulary by using vocabulary of pre-trained word embeddings, such as GloVe.\n",
    "Below are a few pre-trained file names under the GloVe word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.42B.300d',\n",
       " 'glove.6B.100d',\n",
       " 'glove.6B.200d',\n",
       " 'glove.6B.300d',\n",
       " 'glove.6B.50d']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.embedding.list_sources('glove')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For simplicity of demonstration, we use a smaller word embedding file, such as\n",
    "the 50-dimensional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file glove.6B.50d.npz is not found. Downloading from Gluon Repository. This may take some time.\n",
      "Downloading /home/ubuntu/.mxnet/embedding/glove/glove.6B.50d.npz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/glove/glove.6B.50d.npz...\n"
     ]
    }
   ],
   "source": [
    "glove_6b50d = nlp.embedding.create('glove', source='glove.6B.50d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we create vocabulary by using all the tokens from `glove_6b50d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(nlp.data.Counter(glove_6b50d.idx_to_token))\n",
    "vocab.set_embedding(glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Below shows the size of `vocab` including a special unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400004"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can access attributes of `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71424\n",
      "beautiful\n"
     ]
    }
   ],
   "source": [
    "print(vocab['beautiful'])\n",
    "print(vocab.idx_to_token[71424])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To apply word embeddings, we need to define\n",
    "cosine similarity. It can compare similarity of two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "def cos_sim(x, y):\n",
    "    return nd.dot(x, y) / (nd.norm(x) * nd.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The range of cosine similarity between two vectors is between -1 and 1. The\n",
    "larger the value, the similarity between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[-1.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.array([1, 2])\n",
    "y = nd.array([10, 20])\n",
    "z = nd.array([-1, -2])\n",
    "\n",
    "print(cos_sim(x, y))\n",
    "print(cos_sim(x, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given an input word, we can find the nearest $k$ words from\n",
    "the vocabulary (400,000 words excluding the unknown token) by similarity. The\n",
    "similarity between any pair of words can be represented by the cosine similarity\n",
    "of their vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def norm_vecs_by_row(x):\n",
    "    return x / nd.sqrt(nd.sum(x * x, axis=1) + 1E-10).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_knn(vocab, k, word):\n",
    "    word_vec = vocab.embedding[word].reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_vec)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k+1, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    # Remove input tokens.\n",
    "    return vocab.to_tokens(indices[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us find the 5 most similar words of 'baby' from the vocabulary (size:\n",
    "400,000 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['babies', 'boy', 'girl', 'newborn', 'pregnant']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 5, 'baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can verify the cosine similarity of vectors of 'baby' and 'babies'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.83871305]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(vocab.embedding['baby'], vocab.embedding['babies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us find the 5 most similar words of 'computers' from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer', 'phones', 'pcs', 'machines', 'devices']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 5, 'computers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us find the 5 most similar words of 'run' from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['running', 'runs', 'went', 'start', 'ran']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 5, 'run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us find the 5 most similar words of 'beautiful' from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lovely', 'gorgeous', 'wonderful', 'charming', 'beauty']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_knn(vocab, 5, 'beautiful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can also apply pre-trained word embeddings to the word\n",
    "analogy problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For instance, \"man : woman :: son : daughter\" is an analogy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The word analogy completion problem is defined as: for analogy 'a : b :: c : d',\n",
    "given the first three words 'a', 'b', 'c', find 'd'. The idea is to find the\n",
    "most similar word vector for vec('c') + (vec('b')-vec('a')).\n",
    "\n",
    "In this example, we will find words by analogy from the 400,000 indexed words in `vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_k_by_analogy(vocab, k, word1, word2, word3):\n",
    "    word_vecs = vocab.embedding[word1, word2, word3]\n",
    "    word_diff = (word_vecs[1] - word_vecs[0] + word_vecs[2]).reshape((-1, 1))\n",
    "    vocab_vecs = norm_vecs_by_row(vocab.embedding.idx_to_vec)\n",
    "    dot_prod = nd.dot(vocab_vecs, word_diff)\n",
    "    indices = nd.topk(dot_prod.reshape((len(vocab), )), k=k, ret_typ='indices')\n",
    "    indices = [int(i.asscalar()) for i in indices]\n",
    "    return vocab.to_tokens(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Complete word analogy 'man : woman :: son :'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['daughter']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'man', 'woman', 'son')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us verify the cosine similarity between vec('son')+vec('woman')-vec('man')\n",
    "and vec('daughter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[0.9658341]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cos_sim_word_analogy(vocab, word1, word2, word3, word4):\n",
    "    words = [word1, word2, word3, word4]\n",
    "    vecs = vocab.embedding[words]\n",
    "    return cos_sim(vecs[1] - vecs[0] + vecs[2], vecs[3])\n",
    "\n",
    "cos_sim_word_analogy(vocab, 'man', 'woman', 'son', 'daughter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Complete word analogy 'beijing : china :: tokyo : '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['japan']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'beijing', 'china', 'tokyo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Complete word analogy 'bad : worst :: big : '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biggest']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'bad', 'worst', 'big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Complete word analogy 'do : did :: go :'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_k_by_analogy(vocab, 1, 'do', 'did', 'go')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
