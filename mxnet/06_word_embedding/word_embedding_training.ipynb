{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Embeddings Evaluation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:14:10.702945Z",
     "start_time": "2018-08-20T23:14:09.280152Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "\n",
    "# context = mx.cpu()  # Enable this to run on CPU\n",
    "context = mx.gpu(0)  # Enable this to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unknown token handling and subword information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sometimes we may run into a word for which the embedding does not include a word vector.\n",
    "While the `vocab` object is happy to replace it with a special index for unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:14:12.192435Z",
     "start_time": "2018-08-20T23:14:10.704858Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111052, 300)\n"
     ]
    }
   ],
   "source": [
    "pretrained_embedding = nlp.embedding.create('fasttext', source='wiki.simple')\n",
    "print(pretrained_embedding.idx_to_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:14:12.206504Z",
     "start_time": "2018-08-20T23:14:12.194281Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'unknownword' in pretrained_embedding.idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:14:12.216816Z",
     "start_time": "2018-08-20T23:14:12.208270Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.  0.  0.  0.  0.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embedding['unknownword'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We first load pretrained fastText word embeddings.\n",
    "fastText embeddings support computing vectors for unknown words by falling back to vectors learned for ngram level features.\n",
    "In GluonNLP it is possible to specify `load_ngrams=True` when loading pretrained fastText embeddings to load the ngram level features and consequently support meaningful embeddings for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:17.734715Z",
     "start_time": "2018-08-20T23:14:12.218482Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_embedding = nlp.embedding.create('fasttext', source='wiki.en', load_ngrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:18.580499Z",
     "start_time": "2018-08-20T23:15:17.736672Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 0.08307433  0.06700231 -0.25606179  0.16879943 -0.02845737]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embedding['unknownword'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Some embedding models such as the FastText model support computing word vectors for unknown words by taking into account their subword units.\n",
    "\n",
    "- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop , 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantifying Word Embeddings Quality - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The previous example has introduced how to load pre-trained word embeddings\n",
    "from a set of sources included in the Gluon NLP toolkit. It was shown how make\n",
    "use of the word vectors to find the top most similar words of a given words or\n",
    "to solve the analogy task.\n",
    "\n",
    "Besides manually investigating similar words or the predicted analogous words,\n",
    "we can facilitate word embedding evaluation datasets to quantify the\n",
    "evaluation.\n",
    "\n",
    "Datasets for the *similarity* task come with a list of word pairs together with\n",
    "a human similarity judgement. The task is to recover the order of most-similar\n",
    "to least-similar pairs.\n",
    "\n",
    "Datasets for the *analogy* tasks supply a set of analogy quadruples of the form\n",
    "‘a : b :: c : d’ and the task is to recover find the correct ‘d’ in as many\n",
    "cases as possible given just ‘a’, ‘b’, ‘c’. For instance, “man : woman :: son :\n",
    "daughter” is an analogy.\n",
    "\n",
    "The Gluon NLP toolkit includes a set of popular *similarity* and *analogy* task\n",
    "datasets as well as helpers for computing the evaluation scores. Here we show\n",
    "how to make use of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Similarity and Relatedness Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:25.000472Z",
     "start_time": "2018-03-28T20:27:24.993155Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Word embeddings should capture the relationsship between words in natural language.\n",
    "In the Word Similarity and Relatedness Task word embeddings are evaluated by comparing word similarity scores computed from a pair of words with human labels for the similarity or relatedness of the pair.\n",
    "\n",
    "`gluonnlp` includes a number of common datasets for the Word Similarity and Relatedness Task. The included datasets are listed in the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). We use several of them in the evaluation example below.\n",
    "\n",
    "We first show a few samples from the WordSim353 dataset, to get an overall feeling of the Dataset structur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:18.588522Z",
     "start_time": "2018-08-20T23:15:18.582301Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink          mouth          5.96\n",
      "nature         environment    8.31\n",
      "type           kind           8.97\n",
      "wood           forest         7.73\n",
      "Jerusalem      Palestinian    7.65\n",
      "day            summer         3.94\n",
      "lobster        wine           5.7\n",
      "architecture   century        3.78\n",
      "shower         flood          6.03\n",
      "psychology     Freud          8.21\n",
      "money          dollar         8.42\n",
      "impartiality   interest       5.16\n"
     ]
    }
   ],
   "source": [
    "wordsim353 = nlp.data.WordSim353()\n",
    "for i in range(0, len(wordsim353), 30):\n",
    "    print(\"{:<15}{:<15}{}\".format(*wordsim353[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:18.611249Z",
     "start_time": "2018-08-20T23:15:18.590201Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "counter = nlp.data.count_tokens(wordsim353.transform(lambda e: e[0]))\n",
    "counter.update(wordsim353.transform(lambda e: e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:18.642434Z",
     "start_time": "2018-08-20T23:15:18.612840Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vocab_wordsim353 = nlp.Vocab(counter)\n",
    "vocab_wordsim353.set_embedding(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:18.646809Z",
     "start_time": "2018-08-20T23:15:18.644204Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_wordsim353))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Gluon NLP toolkit includes a `WordEmbeddingSimilarity` block, which predicts similarity score between word pairs given an embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:18.653619Z",
     "start_time": "2018-08-20T23:15:18.648449Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=vocab_wordsim353.embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:20.630017Z",
     "start_time": "2018-08-20T23:15:18.655258Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:20.638531Z",
     "start_time": "2018-08-20T23:15:20.631870Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wordsim353_coded = wordsim353.transform(\n",
    "    lambda e: (vocab_wordsim353[e[0]], vocab_wordsim353[e[1]], e[2]))\n",
    "wordsim353_nd = mx.nd.array(wordsim353_coded, ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The similarities can be predicted by passing the two arrays of words through the evaluator. Thereby the *ith* word in `words1` will be compared with the *ith* word in `words2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:21.235703Z",
     "start_time": "2018-08-20T23:15:20.640218Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pred_similarity = evaluator(wordsim353_nd[:, 0], wordsim353_nd[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:21.240985Z",
     "start_time": "2018-08-20T23:15:21.237527Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drink          mouth          0.60      0.30\n",
      "start          match          0.45      0.25\n",
      "development    issue          0.40      0.14\n",
      "volunteer      motto          0.26      0.29\n",
      "money          laundering     0.57      0.59\n",
      "energy         secretary      0.18      0.20\n",
      "midday         noon           0.93      0.70\n",
      "attempt        peace          0.42      0.20\n",
      "psychology     science        0.67      0.57\n",
      "professor      cucumber       0.03      0.10\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2, s), ps in zip(wordsim353[:10], pred_similarity[:10].asnumpy()):\n",
    "    print(\"{:<15}{:<15}{:<10.2f}{:.2f}\".format(w1, w2, s/10, ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can evaluate the predicted similarities, and thereby the word embeddings, by computing the Spearman Rank Correlation between the predicted similarities and the groundtruth, human, similarity scores from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:21.272517Z",
     "start_time": "2018-08-20T23:15:21.243000Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation 0.68\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), wordsim353_nd[:, 2].asnumpy())\n",
    "print('Spearman rank correlation', sr.correlation.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next to making it easy to work with pre-trained word embeddings, `gluonnlp`\n",
    "also provides everything needed to train your own embeddings. Datasets as well\n",
    "as model definitions are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Training data\n",
    "\n",
    "We first load the Text8 corpus from the [Large Text Compression\n",
    "Benchmark](http://mattmahoney.net/dc/textdata.html) which includes the first\n",
    "100 MB of cleaned text from the English Wikipedia. We follow the common practice\n",
    "of splitting every 10'000 tokens to obtain \"sentences\" for embedding training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:23.853831Z",
     "start_time": "2018-08-20T23:15:21.275016Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 1701\n",
      "# tokens: 10000 ['anarchism', 'originated', 'as', 'a', 'term']\n",
      "# tokens: 10000 ['reciprocity', 'qualitative', 'impairments', 'in', 'communication']\n",
      "# tokens: 10000 ['with', 'the', 'aegis', 'of', 'zeus']\n"
     ]
    }
   ],
   "source": [
    "dataset = nlp.data.Text8(segment='train')\n",
    "print('# sentences:', len(dataset))\n",
    "for sentence in dataset[:3]:\n",
    "    print('# tokens:', len(sentence), sentence[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We then build a vocabulary of all the tokens in the dataset that occur more\n",
    "than 5 times and replace the words with their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:42.091350Z",
     "start_time": "2018-08-20T23:15:23.856226Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable(dataset))\n",
    "vocab_training = nlp.Vocab(\n",
    "    counter,\n",
    "    unknown_token=None,\n",
    "    padding_token=None,\n",
    "    bos_token=None,\n",
    "    eos_token=None,\n",
    "    min_freq=5)\n",
    "\n",
    "\n",
    "def code(s):\n",
    "    return [vocab_training[t] for t in s if t in vocab_training]\n",
    "\n",
    "\n",
    "coded_dataset = dataset.transform(code, lazy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sampling distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Subsample frequent words\n",
    "- Sampling distribution $$\\sqrt{f(w_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "where $f(w_i)$ is the frequency with which a word is.\n",
    "\n",
    "Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” Advances in neural information processing systems. 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:42.163636Z",
     "start_time": "2018-08-20T23:15:42.093546Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "subsampling_constant = 1e-5\n",
    "total_count = sum(counter[w] for w in vocab_training.idx_to_token)\n",
    "idx_to_pdiscard = [\n",
    "    1 - math.sqrt(subsampling_constant / (counter[w] / total_count))\n",
    "    for w in vocab_training.idx_to_token\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:42.169459Z",
     "start_time": "2018-08-20T23:15:42.165625Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def subsample(s):\n",
    "    return [\n",
    "        t for t, r in zip(s, np.random.uniform(0, 1, size=len(s)))\n",
    "        if r > idx_to_pdiscard[t]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:45.287799Z",
     "start_time": "2018-08-20T23:15:42.171830Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens for sentences in coded_dataset:\n",
      "9895 [5233, 3083, 11, 5, 194]\n",
      "9858 [18214, 17356, 36672, 4, 1753]\n",
      "9926 [23, 0, 19754, 1, 4829]\n",
      "\n",
      "# tokens for sentences in subsampled_dataset:\n",
      "2955 [5233, 3133, 741, 10619, 27497]\n",
      "2824 [18214, 17356, 36672, 1753, 13001]\n",
      "2751 [19754, 1799, 7069, 950, 8712]\n"
     ]
    }
   ],
   "source": [
    "subsampled_dataset = coded_dataset.transform(subsample, lazy=False)\n",
    "\n",
    "print('# tokens for sentences in coded_dataset:')\n",
    "for i in range(3):\n",
    "    print(len(coded_dataset[i]), coded_dataset[i][:5])\n",
    "\n",
    "print('\\n# tokens for sentences in subsampled_dataset:')\n",
    "for i in range(3):\n",
    "    print(len(subsampled_dataset[i]), subsampled_dataset[i][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Handling subword features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`gluonnlp` provides the concept of a SubwordFunction which maps words to a list of indices representing their subword.\n",
    "Possible SubwordFunctions include mapping a word to the sequence of it's characters/bytes or hashes of all its ngrams.\n",
    "\n",
    "FastText models use a hash function to map each ngram of a word to a number in range `[0, num_subwords)`.\n",
    "We include the same hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:46.742281Z",
     "start_time": "2018-08-20T23:15:45.290879Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<the>\t[151151, 409726, 148960, 361980, 60934, 316280]\n",
      "<of>\t[497102, 164528, 228930]\n",
      "<and>\t[378080, 235020, 30390, 395046, 119624, 125443]\n"
     ]
    }
   ],
   "source": [
    "subword_function = nlp.vocab.create_subword_function(\n",
    "    'NGramHashes', ngrams=[3, 4, 5, 6], num_subwords=500000)\n",
    "\n",
    "idx_to_subwordidxs = subword_function(vocab_training.idx_to_token)\n",
    "for word, subwords in zip(vocab_training.idx_to_token[:3], idx_to_subwordidxs[:3]):\n",
    "    print('<'+word+'>', subwords, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As words are of varying length, we have to pad the lists of subwords to obtain a batch. To distinguish padded values from valid subword indices we use a mask.\n",
    "We first pad the subword arrays with `-1`, compute the mask and change the `-1` entries to some valid subword index (here `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:46.750234Z",
     "start_time": "2018-08-20T23:15:46.744061Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 151151.  409726.  148960.  361980.   60934.  316280.]\n",
      " [ 497102.  164528.  228930.       0.       0.       0.]\n",
      " [ 378080.  235020.   30390.  395046.  119624.  125443.]]\n",
      "<NDArray 3x6 @cpu_shared(0)>\n",
      "\n",
      "[[ 1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  1.  1.]]\n",
      "<NDArray 3x6 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "subword_padding = nlp.data.batchify.Pad(pad_val=-1)\n",
    "\n",
    "subwords = subword_padding(idx_to_subwordidxs[:3])\n",
    "subwords_mask = subwords != -1\n",
    "subwords += subwords == -1  # -1 is invalid. Change to 0\n",
    "print(subwords)\n",
    "print(subwords_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`gluonnlp` provides model definitions for popular embedding models as Gluon Blocks.\n",
    "Here we show how to train them with the Skip-Gram objective, a\n",
    "simple and popular embedding training objective. It was introduced\n",
    "by \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient\n",
    "estimation of word representations in vector space. ICLR Workshop , 2013.\"\n",
    "\n",
    "The Skip-Gram objective trains word vectors such that the word vector of a word\n",
    "at some position in a sentence can best predict the surrounding words. We call\n",
    "these words *center* and *context* words.\n",
    "\n",
    "<img src=\"http://blog.aylien.com/wp-content/uploads/2016/10/skip-gram.png\" width=\"300\">\n",
    "\n",
    "Skip-Gram and picture from \"Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\n",
    "Dean. Efficient estimation of word representations in vector space. ICLR\n",
    "Workshop , 2013.\"\n",
    "\n",
    "\n",
    "For the Skip-Gram objective, we initialize two embedding models: `embedding`\n",
    "and `embedding_out`. `embedding` is used to look up embeddings for the *center*\n",
    "words. `embedding_out` is used for the *context* words.\n",
    "\n",
    "The weights of `embedding` are the final word embedding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:46.753975Z",
     "start_time": "2018-08-20T23:15:46.751966Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "emsize = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:46.759425Z",
     "start_time": "2018-08-20T23:15:46.755613Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "embedding = nlp.model.train.FasttextEmbeddingModel(\n",
    "    token_to_idx=vocab_training.token_to_idx,\n",
    "    subword_function=subword_function,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))\n",
    "embedding_out = nlp.model.train.SimpleEmbeddingModel(\n",
    "    token_to_idx=vocab_training.token_to_idx,\n",
    "    embedding_size=emsize,\n",
    "    weight_initializer=mx.init.Uniform(scale=1 / emsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:46.928803Z",
     "start_time": "2018-08-20T23:15:46.761043Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for e in [embedding, embedding_out]:\n",
    "    e.initialize(ctx=context)\n",
    "    e.hybridize(static_alloc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:47.054123Z",
     "start_time": "2018-08-20T23:15:46.931300Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "params = embedding.collect_params()\n",
    "params.update(embedding_out.collect_params())\n",
    "trainer = mx.gluon.Trainer(params, 'adagrad', dict(learning_rate=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:15:47.101876Z",
     "start_time": "2018-08-20T23:15:47.055983Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "weights = mx.nd.array([counter[w] for w in vocab_training.idx_to_token])**0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.019316Z",
     "start_time": "2018-08-20T23:15:47.103812Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, time 2.43s, iteration 0, throughput=0.84K wps\n",
      "epoch 1, time 20.99s, iteration 500, throughput=48.88K wps\n",
      "epoch 1, time 38.32s, iteration 1000, throughput=53.50K wps\n",
      "epoch 1, time 54.56s, iteration 1500, throughput=56.35K wps\n",
      "epoch 1, time 70.92s, iteration 2000, throughput=57.79K wps\n",
      "epoch 1, time 75.18s, train loss 0.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.train_fasttext_embedding(1, embedding, embedding_out, subsampled_dataset,\n",
    "                         weights, idx_to_subwordidxs, context, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation of trained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We create a new `TokenEmbedding` object and set the embedding vectors for the words we care about for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.578269Z",
     "start_time": "2018-08-20T23:17:08.022628Z"
    }
   },
   "outputs": [],
   "source": [
    "token_embedding = nlp.embedding.TokenEmbedding(unknown_token=None, allow_extend=True)\n",
    "token_embedding[vocab_wordsim353.idx_to_token] = embedding[vocab_wordsim353.idx_to_token]\n",
    "\n",
    "vocab_wordsim353.set_embedding(token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.583764Z",
     "start_time": "2018-08-20T23:17:08.579995Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(\n",
    "    idx_to_vec=vocab_wordsim353.embedding.idx_to_vec,\n",
    "    similarity_function=\"CosineSimilarity\")\n",
    "evaluator.initialize(ctx=context)\n",
    "evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.593281Z",
     "start_time": "2018-08-20T23:17:08.585444Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman rank correlation 0.404\n"
     ]
    }
   ],
   "source": [
    "pred_similarity = evaluator(wordsim353_nd[:, 0], wordsim353_nd[:, 1])\n",
    "sr = stats.spearmanr(pred_similarity.asnumpy(), wordsim353_nd[:, 2].asnumpy())\n",
    "print('Spearman rank correlation', sr.correlation.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice - Quantifying Analogy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T20:27:25.000472Z",
     "start_time": "2018-03-28T20:27:24.993155Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "In the Word Analogy Task word embeddings are evaluated by inferring an analogous word `D`, which is related to a given word `C` in the same way as a given pair of words `A, B` are related.\n",
    "\n",
    "`gluonnlp` includes a number of common datasets for the Word Analogy Task. The included datasets are listed in the [API documentation](http://gluon-nlp.mxnet.io/api/data.html#word-embedding-evaluation-datasets). In this notebook we use the GoogleAnalogyTestSet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.620194Z",
     "start_time": "2018-08-20T23:17:08.595190Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "google_analogy = nlp.data.GoogleAnalogyTestSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We first demonstrate the structure of the dataset by printing a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.628931Z",
     "start_time": "2018-08-20T23:17:08.621907Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "athens              greece              baghdad             iraq                \n",
      "baku                azerbaijan          dushanbe            tajikistan          \n",
      "dublin              ireland             kathmandu           nepal               \n",
      "lusaka              zambia              tehran              iran                \n",
      "rome                italy               windhoek            namibia             \n",
      "zagreb              croatia             astana              kazakhstan          \n",
      "philadelphia        pennsylvania        tampa               florida             \n",
      "wichita             kansas              shreveport          louisiana           \n",
      "shreveport          louisiana           oxnard              california          \n",
      "complete            completely          lucky               luckily             \n",
      "comfortable         uncomfortable       clear               unclear             \n",
      "good                better              high                higher              \n",
      "young               younger             tight               tighter             \n",
      "weak                weakest             bright              brightest           \n",
      "slow                slowing             describe            describing          \n",
      "ireland             irish               greece              greek               \n",
      "feeding             fed                 sitting             sat                 \n",
      "slowing             slowed              decreasing          decreased           \n",
      "finger              fingers             onion               onions              \n",
      "play                plays               sing                sings               \n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(google_analogy), 1000):\n",
    "    print(\"{:<20}{:<20}{:<20}{:<20}\".format(*google_analogy[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Task\n",
    "\n",
    "- Create a vocabulary containing\n",
    "  - the (most frequent) 300000 words of the pretrained embedding\n",
    "  - and all words of the GoogleAnalogyTestSet\n",
    "- Attach the pretrained_embedding to the vocabulary to obtain vectors for all words\n",
    "- Then run below evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.635470Z",
     "start_time": "2018-08-20T23:17:08.632294Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# counter = nlp.data.utils.Counter(...)   # First 300000 entries of pretrained_embedding.idx_to_token\n",
    "# counter.update(itertools.chain.from_iterable(google_analogy))\n",
    "\n",
    "# vocab_google_analogy = nlp.Vocab(...)\n",
    "# vocab_google_analogy.set_embedding(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.640327Z",
     "start_time": "2018-08-20T23:17:08.637573Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# google_analogy_batches = mx.gluon.data.DataLoader(\n",
    "#     google_analogy.transform(vocab_google_analogy.to_indices),\n",
    "#     batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.645558Z",
     "start_time": "2018-08-20T23:17:08.642349Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# evaluator = nlp.embedding.evaluation.WordEmbeddingAnalogy(\n",
    "#     idx_to_vec=vocab_google_analogy.embedding.idx_to_vec,\n",
    "#     exclude_question_words=True,\n",
    "#     analogy_function=\"ThreeCosMul\")\n",
    "# evaluator.initialize(ctx=context)\n",
    "# evaluator.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T23:17:08.650645Z",
     "start_time": "2018-08-20T23:17:08.647656Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# acc = mx.metric.Accuracy()\n",
    "\n",
    "# for batch in google_analogy_batches:\n",
    "#     batch = batch.as_in_context(context)\n",
    "#     pred_idxs = evaluator(batch[:, 0], batch[:, 1], batch[:, 2])\n",
    "#     acc.update(pred_idxs[:, 0], batch[:, 3])\n",
    "\n",
    "# print('Accuracy', acc.get()[1].round(3))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
