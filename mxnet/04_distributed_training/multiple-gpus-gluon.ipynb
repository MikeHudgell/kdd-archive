{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Training on Multiple GPUs with `gluon`\n",
    "\n",
    "In this notebook, we will\n",
    "- introduce how MXNet handles parallelism\n",
    "- implement data parallel training for LeNet (GPUs required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 20 20:39:24 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   40C    P0    24W / 300W |      0MiB / 16152MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   40C    P0    24W / 300W |      0MiB / 16152MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   39C    P0    24W / 300W |      0MiB / 16152MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   41C    P0    25W / 300W |      0MiB / 16152MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check the number of GPUs available\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Does MXNet Handle Parallelism?\n",
    "\n",
    "Wiriting parallel code in Python is non-trivial. What about in MXNet?\n",
    "\n",
    "\n",
    "In MXNet, operations are\n",
    "\n",
    "- executed asynchronously, and\n",
    "\n",
    "- scheduled according to dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Asynchronous Execution\n",
    "\n",
    "- Operations are pushed to the backend engine and executed asynchronously.\n",
    "\n",
    "- The python code is blocked only when `print(y)` or `y.asnumpy()` is called and the result is not ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Workloads, such as nd.dot are pushed into the backend engine for lazy evaluation. That is, Python merely pushes the workload nd.dot and returns immediately without waiting for the computation to be finished. We keep pushing until the results need to be copied out from MXNet, such as print(x) or are converted into numpy by x.asnumpy(). At that time, the Python thread is blocked until the results are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "from time import time\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations are pushed into the backend engine\n",
      "0.002873 s\n",
      "Operations are done and the result is ready\n",
      "0.217274 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "i = nd.random.uniform(shape=(2000,2000))\n",
    "j = nd.dot(i, i.T)\n",
    "print('Operations are pushed into the backend engine\\n%f s' % (time() - start))\n",
    "j.asnumpy()\n",
    "print('Operations are done and the result is ready\\n%f s' % (time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependency Scheduling\n",
    "\n",
    "Independent operations may be scheduled to run in parallel by MXNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "MXNet depends on a powerful scheduling algorithm that analyzes the dependencies of the pushed workloads. This scheduler checks to see if two workloads are independent of each other. If they are, then the engine may run them in parallel. If a workload depend on results that have not yet been computed, it will be made to wait until its inputs are ready.\n",
    "\n",
    "For example, if we call three operators:\n",
    "```\n",
    "a = nd.random_uniform(...)\n",
    "b = nd.random_uniform(...)\n",
    "c = a + b\n",
    "```\n",
    "Then the computation for a and b may run in parallel, while c cannot be computed until both a and b are ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Defining NDArrays and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create NDArrays on GPU\n",
    "x0 = nd.random.uniform(shape=(4000, 4000), ctx=mx.gpu(0))\n",
    "x1 = nd.random.uniform(shape=(4000, 4000), ctx=mx.gpu(1))\n",
    "\n",
    "def run(x):\n",
    "    \"\"\"push 10 matrix-matrix multiplications\"\"\"\n",
    "    return [nd.dot(x,x) for i in range(10)]\n",
    "\n",
    "def wait(x):\n",
    "    \"\"\"explicitly wait until all results are ready\"\"\"\n",
    "    for y in x:\n",
    "        y.wait_to_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Comparing Sequential and Parallel Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU 0 and 1 in sequential\n",
      "time: 0.245280 sec\n",
      "Run on GPU 0 and 1 in parallel\n",
      "time: 0.106554 sec\n"
     ]
    }
   ],
   "source": [
    "print('Run on GPU 0 and 1 in sequential')\n",
    "start = time()\n",
    "wait(run(x0))\n",
    "wait(run(x1))\n",
    "print('time: %f sec' %(time() - start))\n",
    "print('Run on GPU 0 and 1 in parallel')\n",
    "start = time()\n",
    "y0 = run(x0)\n",
    "y1 = run(x1)\n",
    "wait(y0)\n",
    "wait(y1)\n",
    "print('time: %f sec' %(time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Parallelism for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For deep learning, data parallelism is by far the most widely used approach for partitioning workloads. It works like this: Assume that we have k GPUs. We split the examples in a data batch into k parts, and send each part to a different GPUs which then computes the gradient that part of the batch. Finally, we collect the gradients from each of the GPUs and sum them together before updating the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"data_parallel.png\" width='800px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training ResNet34 V2 with Multiple GPUs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Define the neural network and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = gluon.model_zoo.vision.resnet34_v2(classes=10)\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Initializing on Multiple Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Gluon supports initialization of network parameters over multiple devices. We accomplish this by passing in an array of device contexts, instead of the single contexts we've used in earlier notebooks.\n",
    "When we pass in an array of contexts, the parameters are initialized \n",
    "to be identical across all of our devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(0), mx.gpu(1)]\n",
    "num_devices = len(ctx)\n",
    "net.hybridize(static_alloc=True, static_shape=True)\n",
    "net.initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Given a batch of input data,\n",
    "we can split it into parts (equal to the number of contexts) \n",
    "by calling `gluon.utils.split_and_load(batch, ctx)`.\n",
    "The `split_and_load` function doesn't just split the data,\n",
    "it also loads each part onto the appropriate device context. \n",
    "\n",
    "So now when we call the forward pass on two separate parts,\n",
    "each one is computed on the appropriate corresponding device and using the version of the parameters stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mnist_train = utils.normalize_and_copy(gluon.data.vision.MNIST(train=True), ctx=ctx[1])\n",
    "mnist_test = utils.normalize_and_copy(gluon.data.vision.MNIST(train=False), ctx=ctx[1])\n",
    "batch_size = 64\n",
    "train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Split Data for Forward and Backward Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.18367606  0.38539067  0.0810388  -0.36636704  0.24176018 -0.12539324\n",
      "   0.0618905   0.21979626  0.3648352   0.27388057]\n",
      " [-0.1568249   0.560107    0.20823881 -0.58401716  0.5127848   0.00183654\n",
      "   0.21796837  0.22614041  0.4221663   0.36251718]]\n",
      "<NDArray 2x10 @gpu(0)>\n",
      "\n",
      "[[-0.09060284  0.3859282   0.3972354  -0.7385969   0.34928346  0.01967236\n",
      "  -0.02816124  0.43405226  0.3601955   0.20923229]\n",
      " [-0.07137774  0.23342773  0.13774091 -0.4162132   0.35673463  0.01140356\n",
      "   0.09675159  0.1791319   0.22318979  0.21127512]]\n",
      "<NDArray 2x10 @gpu(1)>\n"
     ]
    }
   ],
   "source": [
    "i, (data, label) = next(enumerate(train_data))\n",
    "data_list = gluon.utils.split_and_load(data[:4], ctx)\n",
    "label_list = gluon.utils.split_and_load(label[:4], ctx)\n",
    "print(net(data_list[0]))\n",
    "print(net(data_list[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At any time, we can access the version of the parameters stored on each device. \n",
    "Recall from the first Chapter that our weights may not actually be initialized\n",
    "when we call `initialize` because the parameter shapes may not yet be known. \n",
    "In these cases, initialization is deferred pending shape inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inspect Weights of the First Layer on Different Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== channel 0 of the first conv on gpu(0) ===\n",
      "[[[ 0.03582338 -0.05703442  0.02046952  0.00537236  0.01164322\n",
      "   -0.01281808  0.03641203]\n",
      "  [-0.03176162 -0.060988    0.06300376 -0.03806128 -0.02654165\n",
      "   -0.00634275  0.02375414]\n",
      "  [ 0.00347787  0.06492888  0.00712219  0.0277181   0.00173751\n",
      "    0.0262342  -0.01722588]\n",
      "  [ 0.03967453 -0.06445411  0.02099127  0.05131169 -0.01271829\n",
      "   -0.02123801 -0.03724146]\n",
      "  [ 0.01555783  0.0639345   0.01833989  0.02040232  0.02875151\n",
      "   -0.06902379  0.01204208]\n",
      "  [-0.01736174  0.02835646 -0.06423311  0.05263435  0.05001806\n",
      "    0.06768566 -0.03909312]\n",
      "  [ 0.03614714  0.005475    0.03496752 -0.041912    0.05475691\n",
      "    0.01937268  0.00913627]]]\n",
      "<NDArray 1x7x7 @gpu(0)>\n",
      "=== channel 0 of the first conv on gpu(1) ===\n",
      "[[[ 0.03582338 -0.05703442  0.02046952  0.00537236  0.01164322\n",
      "   -0.01281808  0.03641203]\n",
      "  [-0.03176162 -0.060988    0.06300376 -0.03806128 -0.02654165\n",
      "   -0.00634275  0.02375414]\n",
      "  [ 0.00347787  0.06492888  0.00712219  0.0277181   0.00173751\n",
      "    0.0262342  -0.01722588]\n",
      "  [ 0.03967453 -0.06445411  0.02099127  0.05131169 -0.01271829\n",
      "   -0.02123801 -0.03724146]\n",
      "  [ 0.01555783  0.0639345   0.01833989  0.02040232  0.02875151\n",
      "   -0.06902379  0.01204208]\n",
      "  [-0.01736174  0.02835646 -0.06423311  0.05263435  0.05001806\n",
      "    0.06768566 -0.03909312]\n",
      "  [ 0.03614714  0.005475    0.03496752 -0.041912    0.05475691\n",
      "    0.01937268  0.00913627]]]\n",
      "<NDArray 1x7x7 @gpu(1)>\n"
     ]
    }
   ],
   "source": [
    "weight = net.features[1].weight\n",
    "for c in ctx:\n",
    "    weight_on_ctx = weight.data(ctx=c)\n",
    "    print('=== channel 0 of the first conv on {} ==={}'.format(\n",
    "        c, weight_on_ctx[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inspect Gradients of the First Layer on Different Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def forward_backward(net, data_list, label_list):\n",
    "    with autograd.record():\n",
    "        losses = [loss(net(X), Y) for X, Y in zip(data_list, label_list)]\n",
    "    for l in losses:\n",
    "        l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similarly, we can access the gradients on each of the GPUs. Because each GPU gets a different part of the batch (a different subset of examples), the gradients on each GPU vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== grad of channel 0 of the first conv2d on gpu(0) ===\n",
      "[[[  3517.4688 -44901.09   -89587.13   -66705.73    20949.812\n",
      "    53379.555   57688.137 ]\n",
      "  [ 31140.346  -16102.486  -44485.695  -43412.918   26441.758\n",
      "    73986.58    31593.041 ]\n",
      "  [ 49982.047   10538.945  -12221.834  -32302.422   50121.273\n",
      "    66072.21    21097.793 ]\n",
      "  [ 47142.953    7937.446   -9989.615   -3465.6895  -2173.338\n",
      "    40656.98     5405.1484]\n",
      "  [ 75860.35    38860.39   -11847.77   -20935.316  -10120.942\n",
      "    30186.033    5465.3755]\n",
      "  [ 52975.566   16579.865   -8577.469  -33620.63   -21547.572\n",
      "    -9651.623   -9236.727 ]\n",
      "  [ 43617.996   50427.5      6384.574  -36300.83   -35787.734\n",
      "   -42527.055  -36163.684 ]]]\n",
      "<NDArray 1x7x7 @gpu(0)>\n",
      "=== grad of channel 0 of the first conv2d on gpu(1) ===\n",
      "[[[ 2.9299689e+04  1.8483438e+04  8.5118613e+03  6.6304614e+02\n",
      "    3.5992820e+04  2.3011988e+04  1.6259367e+04]\n",
      "  [ 1.8721021e+04  2.3937648e+04 -2.4922852e+01 -1.7493465e+04\n",
      "    1.7566496e+04 -1.2260168e+03 -2.1855119e+04]\n",
      "  [-5.1907574e+04 -1.2440020e+04 -3.2326084e+03 -5.3757227e+03\n",
      "    1.1034203e+04 -2.5414326e+04 -4.1674703e+04]\n",
      "  [ 2.6597676e+04 -1.4546712e+04 -1.6811893e+04 -1.7496578e+04\n",
      "   -2.8691535e+04 -6.8872539e+03 -1.3583857e+03]\n",
      "  [ 4.8581039e+04  3.5666852e+04  5.0819932e+03  1.2468238e+04\n",
      "    1.3528551e+04  9.2854072e+03  6.1416875e+04]\n",
      "  [ 1.5732339e+03  3.2380801e+04  5.0236441e+04  4.1008641e+04\n",
      "   -7.1869805e+03  8.1493066e+02  9.3319141e+02]\n",
      "  [-7.4709658e+03 -1.3719197e+04  2.2257707e+04 -6.8279805e+03\n",
      "   -3.6568965e+04 -3.2129564e+04 -1.9990088e+04]]]\n",
      "<NDArray 1x7x7 @gpu(1)>\n"
     ]
    }
   ],
   "source": [
    "forward_backward(net, data_list, label_list)\n",
    "for c in ctx:\n",
    "    grad_on_ctx = weight.grad(ctx=c)\n",
    "    print('=== grad of channel 0 of the first conv2d on {} ==={}'.format(\n",
    "        c, grad_on_ctx[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Put all things together\n",
    "\n",
    "Now we can implement the remaining functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Define Train and Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(data, label, ctx, net, trainer):\n",
    "    # split the data batch and load them on GPUs\n",
    "    data_list = gluon.utils.split_and_load(data, ctx)\n",
    "    label_list = gluon.utils.split_and_load(label, ctx)\n",
    "    # compute gradient\n",
    "    forward_backward(net, data_list, label_list)\n",
    "    # update parameters\n",
    "    trainer.step(data.shape[0])\n",
    "\n",
    "def valid_batch(data, label, ctx, net):\n",
    "    data = data.as_in_context(ctx[0])\n",
    "    pred = nd.argmax(net(data), axis=1, keepdims=True)\n",
    "    return nd.sum(pred == label.as_in_context(pred.context)).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def run(ctx, batch_size, lr):    \n",
    "    # data iterator\n",
    "    train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle=True)\n",
    "    valid_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle=False)\n",
    "    print('Batch size is {}'.format(batch_size))\n",
    "    net.collect_params().initialize(force_reinit=True, ctx=ctx)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    for epoch in range(2):\n",
    "        # train\n",
    "        start = time()\n",
    "        for data, label in train_data:\n",
    "            train_batch(data, label, ctx, net, trainer)\n",
    "        mx.nd.waitall()\n",
    "        print('Epoch %d, training time = %.1f sec'%(epoch, time()-start))\n",
    "        # validating\n",
    "        correct, num = 0.0, 0.0\n",
    "        for data, label in valid_data:\n",
    "            correct += valid_batch(data, label, ctx, net)\n",
    "            num += data.shape[0]\n",
    "        print('         validation accuracy = %.4f'%(correct/num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on [gpu(0)]\n",
      "Batch size is 128\n",
      "Epoch 0, training time = 10.3 sec\n",
      "         validation accuracy = 0.9879\n",
      "Epoch 1, training time = 9.5 sec\n",
      "         validation accuracy = 0.9859\n",
      "Running on [gpu(0), gpu(1)]\n",
      "Batch size is 256\n",
      "Epoch 0, training time = 8.5 sec\n",
      "         validation accuracy = 0.9498\n",
      "Epoch 1, training time = 7.6 sec\n",
      "         validation accuracy = 0.9841\n"
     ]
    }
   ],
   "source": [
    "# single GPU\n",
    "ctx_list = [ctx[0]]\n",
    "print('Running on {}'.format(ctx_list))\n",
    "run(ctx_list, 128*len(ctx_list), .04)\n",
    "# multi-GPU\n",
    "ctx_list = ctx\n",
    "print('Running on {}'.format(ctx_list))\n",
    "run(ctx_list, 128*len(ctx_list), .08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Gluon makes it easy to implement data parallel training.\n",
    "\n",
    "Both parameters and trainers in `gluon` support multi-devices. Moving from one device to multi-devices is straightforward. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
