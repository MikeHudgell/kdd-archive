{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentiment Analysis (SA) with Pre-trained Language Model (LM)\n",
    "\n",
    "In this notebook, we demonstrate how to analyze sentiments of IMDB reviews using a pre-trained language model.\n",
    "\n",
    "- Model definition\n",
    "- Data pipeline\n",
    "- Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now that we've covered some advanced topics, let's go back and show how these techniques can help us even when addressing the comparatively simple problem of classification. In particular, we'll look at the classic problem of sentiment analysis: taking an input consisting of a string of text and classifying its sentiment as positive of negative.\n",
    "\n",
    "In this notebook, we are going to use GluonNLP to build a sentiment analysis model whose weights are initialized based on a pretrained language model. Using pre-trained language model weights is a common approach for semi-supervised learning in NLP. In order to do a good job with language modeling on a large corpus of text, our model must learn representations that contain information about the structure of natural language. Intuitively, by starting with these good features, vs random features, we're able to converge faster upon a good model for our downsteam task.\n",
    "\n",
    "With GluonNLP, we can quickly prototype the model and it's easy to customize. The building process consists of just three simple steps. For this demonstration we'll focus on movie reviews from the Large Movie Review Dataset, also known as the IMDB dataset. Given a movie, our model will output prediction of its sentiment, which can be positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can easily transplant the pre-trained weights, we'll base our model architecture on the pre-trained LM. Following the LSTM layer, we have one representation vector for each word in the sentence. Because we plan to make a single prediction (not one per word), we'll first pool our predictions across time steps before feeding them through a dense layer to produce our final prediction (a single sigmoid output node)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='samodel-v3.png' width='250px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dense: make predictions\n",
    "\n",
    "Pooling: downsampling\n",
    "\n",
    "Embedding: from LM\n",
    "\n",
    "Encoder: from LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "Specifically, our model represents input words by their embeddings. Following the embedding layer, our model consists of a two-layer LSTM, followed by an average pooling layer, followed by a sigmoid output layer (all illustrated in the figure above)\n",
    "\n",
    "Thus, given an input sequence, the memory cells in the LSTM layer will produce a representation sequence. This representation sequence is then averaged over all timesteps resulting in a fixed-length sentence representation $h$. Finally, we apply a sigmoid output layer on top of $h$. We’re using the sigmoid  because we’re trying to predict if this text has positive or negative sentiment, and a sigmoid activation function squashes the output values to the range [0,1], allowing us to interpret this output as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Definition in GluonNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "import utils\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MeanPoolingLayer(mx.gluon.HybridBlock):\n",
    "    \"\"\"A block for mean pooling of encoder features\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(MeanPoolingLayer, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length):\n",
    "        # Data will have shape (T, N, C)\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n",
    "                                    F.expand_dims(valid_length, axis=1))\n",
    "        return agg_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None # will set with lm embedding later\n",
    "            self.encoder = None   # will set with lm encoder later\n",
    "            self.agg_layer = MeanPoolingLayer()\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length):\n",
    "        # shape Shape = (T, N, C)\n",
    "        encoded = self.encoder(self.embedding(data))\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Hyperparameters and Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load Pre-trained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "context = mx.gpu(0)\n",
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create SA model from Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the below code, we first acquire a pre-trained model on the Wikitext-2 dataset using nlp.model.get_model. We then construct a SentimentNet object, which takes as input the embedding layer and encoder of the pre-trained model.\n",
    "\n",
    "As we employ the pre-trained embedding layer and encoder, **we only need to initialize the output layer** using `net.out_layer.initialize(mx.init.Xavier(), ctx=context)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "batch_size = 16\n",
    "epochs = 1\n",
    "\n",
    "net = SentimentNet()\n",
    "net.embedding = lm_model.embedding\n",
    "net.encoder = lm_model.encoder\n",
    "net.hybridize()\n",
    "# initialize only the output layer\n",
    "net.output.initialize(mx.init.Xavier(), ctx=context)\n",
    "loss = gluon.loss.SigmoidBCELoss()\n",
    "trainer = gluon.Trainer(net.collect_params(),'ftml',\n",
    "                        {'learning_rate': learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (agg_layer): MeanPoolingLayer(\n",
      "  \n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Preparation\n",
    "The data preprocessing logic depends on English spaCy tokenizer. If you are not running this example on the provided AMI, please add a cell and run the following command:\n",
    "\n",
    "```bash\n",
    "!python -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Glance at the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:\t 8\n",
      "\n",
      "review:\n",
      "\n",
      " I was fortunate to attend the London premier of this film. While I am not at all a fan of British drama, I did find myself deeply moved by the characters and the BAD CHOICES they made. I was in tears by the end of the film. Every scene was mesmerizing. The attention to detail and the excellent acting was quite impressive.<br /><br />I would have to agree with some of the other comments here which question why all these women were throwing themselves at such a despicable character.<br /><br />*******SPOLIER ALERT******** I was also hoping that Dylan would have been killed by William when he had the chance! ****END SPOILER*****<br /><br />Keira Knightley did a great job and radiate beauty and innocence from the screen, but it was Sienna Miller's performance that was truly Oscar worthy.<br /><br />I am sure this production will be nominated for other awards.\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset = nlp.data.IMDB(root='data/imdb', segment='train')\n",
    "raw_test_dataset = nlp.data.IMDB(root='data/imdb', segment='test')\n",
    "print('score:\\t', raw_train_dataset[11][1])\n",
    "print('\\nreview:\\n\\n', raw_test_dataset[11][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need to do the following to preprocess the dataset:\n",
    "\n",
    "- Tokenization\n",
    "- Label generation\n",
    "- Batching with bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Tokenization and Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "def preprocess(x):\n",
    "    # length_clip takes as input a list \n",
    "    # and outputs a list with maximum length 500.\n",
    "    length_clip = nlp.data.ClipSequence(500)\n",
    "    data, label = x\n",
    "    label = int(label > 5)\n",
    "    # A token index or a list of token indices is\n",
    "    # returned according to the vocabulary.\n",
    "    data = vocab[length_clip(tokenizer(data))]\n",
    "    return data, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Preprocess the Dataset with Multi-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done tokenization. Time = 5.57s, num sentences = 25000\n",
      "Done tokenization. Time = 7.35s, num sentences = 25000\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done tokenization. Time = {:.2f}s, num sentences = {}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "train_dataset, train_data_lengths = preprocess_dataset(raw_train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(raw_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Batchify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the following code, we use FixedBucketSampler, which assigns each data sample to a fixed bucket based on its length. The bucket keys are either given or generated from the input sequence lengths and the number of buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=25000, batch_num=1551\n",
      "  key=[59, 108, 157, 206, 255, 304, 353, 402, 451, 500]\n",
      "  cnt=[590, 1999, 5092, 5102, 3038, 2085, 1477, 1165, 870, 3582]\n",
      "  batch_size=[27, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "    nlp.data.batchify.Stack(dtype='float32'))\n",
    "batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "    train_data_lengths,\n",
    "    batch_size=batch_size,\n",
    "    num_buckets=bucket_num,\n",
    "    ratio=bucket_ratio,\n",
    "    shuffle=True)\n",
    "print(batch_sampler.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = gluon.data.DataLoader(dataset=train_dataset,\n",
    "                                         batch_sampler=batch_sampler,\n",
    "                                         batchify_fn=batchify_fn)\n",
    "\n",
    "test_dataloader = gluon.data.DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        batchify_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        train_avg_L, train_throughput = utils.train_one_epoch(epoch, trainer, train_dataloader, \n",
    "                                                              net, loss, context)\n",
    "        test_avg_L, test_acc = utils.evaluate(net, test_dataloader, context)\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, '\n",
    "              'test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                  epoch, train_avg_L, test_acc, test_avg_L, train_throughput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 300/1551] elapsed 13.41 s, avg loss 0.002270, throughput 87.28K wps\n",
      "[Epoch 0 Batch 600/1551] elapsed 12.97 s, avg loss 0.001673, throughput 95.13K wps\n",
      "[Epoch 0 Batch 900/1551] elapsed 12.38 s, avg loss 0.001376, throughput 103.13K wps\n",
      "[Epoch 0 Batch 1200/1551] elapsed 14.13 s, avg loss 0.001504, throughput 79.35K wps\n",
      "[Epoch 0 Batch 1500/1551] elapsed 13.15 s, avg loss 0.001342, throughput 87.78K wps\n",
      "Begin Testing...\n",
      "[Batch 400/1563] elapsed 16.42 s\n",
      "[Batch 800/1563] elapsed 16.65 s\n",
      "[Batch 1200/1563] elapsed 16.85 s\n",
      "[Epoch 0] train avg loss 0.001623, test acc 0.87, test avg loss 0.330541, throughput 90.31K wps\n"
     ]
    }
   ],
   "source": [
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate with Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.7197244]]\n",
       "<NDArray 1x1 @gpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = ['This', 'movie', 'is', 'amazing']\n",
    "test_review = mx.nd.array(vocab[sample], ctx=context)\n",
    "test_length = mx.nd.array([4], ctx=context)\n",
    "net(test_review.reshape(-1, 1), test_length).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practice\n",
    "\n",
    "- Try with a negative sample. Does the network correctly predict the sentiment?\n",
    "- Try re-initialize the network without pre-trained model. Does pre-trained model provide any advantage?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
