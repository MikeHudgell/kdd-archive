{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Language Modeling using GluonNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this notebook, we will use GluonNLP to train a pre-defined LSTM language model on a corpus of real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A statistical model is simple a probability distribution over sequences of words or characters [1].\n",
    "In this tutorial, we'll restrict our attention to word-based language models.\n",
    "Given a reliable language model we can answer questions like \n",
    "*which among the following strings are we more likely to encounter?*\n",
    "\n",
    "1. 'On Monday, Mr. Lamar’s “DAMN.” took home an even more elusive honor, one that may never have even seemed within reach: the Pulitzer Prize\" \n",
    "1. \"Frog zealot flagged xylophone the bean wallaby anaphylaxis extraneous porpoise into deleterious carrot banana apricot.\"\n",
    "\n",
    "Even if we've never seen either of these sentences in our entire lives, and even though no rapper has previously been awarded a Pulitzer Prize, we wouldn't be shocked to see the first sentence in the New York Times. By comparison, we can all agree that the second sentence, consisting of incoherent babble, is comparatively unlikely. \n",
    "A statistical language model can assign precise probabilities to each string of words.\n",
    "\n",
    "Given a large corpus of text, we can estimate (i.e., train) a language model $\\hat{p}(x_1, ..., x_n)$. And given such a model, we can sample strings $\\mathbf{x} \\sim \\hat{p}(x_1, ..., x_n)$, generating new strings according to their estimated probability. Among other useful applications, we can use language models to score candidate transcriptions from speech recognition models, given preference to sentences that seem more probable (at the expense of those deemed anomalous).\n",
    "\n",
    "These days recurrent neural networks (RNNs) are the preferred method for LM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Language model definition - one sentence\n",
    "\n",
    "The standard approach to language modeling consists of training a model that given a trailing window of text, predicts  the next word in the sequence. When we train the model we feed in the inputs $x1, x_2, ...$ and try at each time step to predict the corresponding next word $x_2, ..., x_{n+1}$. To generate text from a language model, we can iteratively predict the next word, and then feed this word as the input to the model at the subsequent time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Predict the next word based on the sequence history, measured by perplexity (surprise).\n",
    "<img src='https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/rnnlm.png' width='700px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train your own language model\n",
    "Now let's train a language model with GluonNLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Preparation\n",
    "We'll start by taking care of our basic dependencies and setting up our environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Load gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "from utilities import detach, train_one_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64 * len(context)\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Load dataset, extract vocabulary, numericalize, and batchify for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "train_dataset, val_dataset, test_dataset = [\n",
    "    nlp.data.WikiText2(segment=segment,\n",
    "                       bos=None, eos='<eos>',\n",
    "                       skip_empty=False)\n",
    "    for segment in ['train', 'val', 'test']]\n",
    "\n",
    "vocab = nlp.Vocab(nlp.data.Counter(train_dataset),\n",
    "                  padding_token=None, bos_token=None)\n",
    "\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "    vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = [\n",
    "    bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Load pre-defined language model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 33278, linear)\n",
      "  )\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None)\n",
    "print(model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Intialize Paramter and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr,\n",
    "                         'momentum': 0,\n",
    "                         'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now that everything is ready, we can start training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluation\n",
    "\n",
    "States are carried over through time.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/e/ee/Unfold_through_time.png width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        # with autograd.record(): -> only needed for training\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        # hidden = detach(hidden) -> only needed for training\n",
    "        L = loss(output.reshape(-3, -1),\n",
    "                 target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Training loop\n",
    "\n",
    "Our loss function will be the standard cross-entropy loss function used for multiclass classification,\n",
    "applied at each time step to compare our predictions to the true next word in the sequence.\n",
    "We can calculate gradients with respect to our parameters using truncated [back-propagation-through-time (BPTT)](https://en.wikipedia.org/wiki/Backpropagation_through_time). \n",
    "In this case, we'll backpropagate for $35$ time steps, updating our weights with stochastic gradient descent with the learning rate of $20$, hyperparameters that we chose earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, batch_size, grad_clip, log_interval, loss, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        start_epoch_time = time.time()\n",
    "        train_one_epoch(epoch, model, train_data, batch_size, grad_clip,\n",
    "                        log_interval, loss, parameters, trainer, context)        \n",
    "        mx.nd.waitall()\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) / \n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/932] loss 7.65, ppl 2097.86, throughput 1511.51 samples/s\n",
      "[Epoch 0 Batch 400/932] loss 6.74, ppl 844.45, throughput 1573.54 samples/s\n",
      "[Epoch 0 Batch 600/932] loss 6.33, ppl 563.60, throughput 1528.89 samples/s\n",
      "[Epoch 0 Batch 800/932] loss 6.16, ppl 471.36, throughput 1472.79 samples/s\n",
      "[Epoch 0] throughput 1531.30 samples/s\n",
      "[Epoch 0] time cost 43.20s, valid loss 5.90, valid ppl 365.83\n",
      "test loss 5.81, test ppl 334.62\n",
      "[Epoch 1 Batch 200/932] loss 5.92, ppl 372.51, throughput 1515.07 samples/s\n",
      "[Epoch 1 Batch 400/932] loss 5.77, ppl 320.56, throughput 1529.00 samples/s\n",
      "[Epoch 1 Batch 600/932] loss 5.62, ppl 275.64, throughput 1526.68 samples/s\n",
      "[Epoch 1 Batch 800/932] loss 5.56, ppl 259.94, throughput 1525.26 samples/s\n",
      "[Epoch 1] throughput 1522.00 samples/s\n",
      "[Epoch 1] time cost 43.43s, valid loss 5.42, valid ppl 226.65\n",
      "test loss 5.34, test ppl 207.50\n",
      "[Epoch 2 Batch 200/932] loss 5.46, ppl 235.34, throughput 1524.60 samples/s\n",
      "[Epoch 2 Batch 400/932] loss 5.38, ppl 216.97, throughput 1466.43 samples/s\n",
      "[Epoch 2 Batch 600/932] loss 5.28, ppl 196.84, throughput 1524.45 samples/s\n",
      "[Epoch 2 Batch 800/932] loss 5.27, ppl 194.50, throughput 1530.35 samples/s\n",
      "[Epoch 2] throughput 1523.23 samples/s\n",
      "[Epoch 2] time cost 43.43s, valid loss 5.22, valid ppl 184.99\n",
      "test loss 5.14, test ppl 170.49\n",
      "Total training throughput 1242.35 samples/s\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, val_data, test_data, batch_size, grad_clip, log_interval, loss, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- GluonNLP provides high-level APIs that could drastically simplify the development process of modeling for NLP tasks.\n",
    "- Low-level APIs in GluonNLP enables easy customization.\n",
    "\n",
    "Documentation can be found at http://gluon-nlp.mxnet.io/index.html\n",
    "\n",
    "Code is here https://github.com/dmlc/gluon-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Reference\n",
    "[1] https://en.wikipedia.org/wiki/Language_model\n",
    "\n",
    "[2] Merity, S., et al. “Regularizing and optimizing LSTM language models”. ICLR 2018"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "rise": {"scroll": true}
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
