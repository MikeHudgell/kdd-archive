{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Large Scale Language Model with Softmax Approximation\n",
    "\n",
    "We'll show you how to define a language model which performs softmax approximation during training, and calculates exact softmax for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the [previous notebook](language_model.ipynb), \n",
    "we demonstrated how you can quickly get up and running, \n",
    "using GluonNLP to implement your own language models \n",
    "(or to grab popular off-the-shelf architectures), \n",
    "and on a variety of datasets. \n",
    "\n",
    "However, we left out one sticky issue that complicates the application of neural network language models to large-scale problems. The size of a neural network's softmax output layer grows with the size of the vocabulary. \n",
    "For vocabularies with 100s of thousands of words, we can quickly find ourselves dealing with output matrices containing hundreds of millions of parameters. \n",
    "Moreover, while we've seen how to get around this computational difficulty on the input side [by using word embeddings](../06_word_embedding/word_embedding.ipynb), we actually need to compute the logits for each word at each time step in order to compute the partition function. \n",
    "\n",
    "\n",
    "In this notebook, we'll teach you about some techniques for approximating the softmax distribution, using importance weighting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Problem of Language Models with Large Vocabularies\n",
    "\n",
    "As we just described, when a word-level language model is trained on a corpus with large vocabulary size, \n",
    "the output layer easily becomes the bottleneck. \n",
    "\n",
    "<img src=\"softmax.png\" width=\"500\">\n",
    "\n",
    "![title](archi.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Approach:\n",
    "- Sampled softmax for training\n",
    "- Full softmax for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Specifically, for each time step our language model must calculate \n",
    "the (un-normalized) probabilities assigned to *each word **w** in the vocabulary*,  due to the need to calculate the partition function (denominator) of the softmax function, which requires the logits of each word _**z(w)**_. \n",
    "\n",
    "Calculating the exact softmax could consume significant memory and computation. \n",
    "For example, the Google Billion Words(GBW) <sup>[1]</sup> dataset has a vocabulary size of ~800K. \n",
    "For a mini-batch of 256 setences unrolled 20 time steps each, the output of softmax layer alone \n",
    "takes 256 \\* 20 \\* 800K \\* 4 bytes = 12.8 GB of memory.\n",
    "\n",
    "One way to combat this issue is to employ a sampling-based scheme for approximating the gradient for the output layer.\n",
    "We focus here on the importance-sampling scheme introduced by Bengio et al. <sup>[2]</sup>. \n",
    "The importance sampling approach trains a multi-label classifier to discriminate between true data, \n",
    "or samples from some proposal distribution. \n",
    "Instead of calculating logits for all classes, \n",
    "only those for the true data and K sampled classes are needed during training.\n",
    "\n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preparation\n",
    "\n",
    "#### Load gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp import data, model\n",
    "from sampler import LogUniformSampler\n",
    "from utilities import detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Define model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this code block, we define the embedding, encoder and decoder layers for the model. Note that `ISLogits` calculates importance sampling based logits and is used for training, while\n",
    "`Dense` is used for testing. The parameters are shared by both layers.\n",
    "\n",
    "Besides `ISLogits`, Gluon NLP also provides `NCELogits` to assist calculate NCE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, num_sampled, dropout=0., **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(vocab_size, num_embed, weight_initializer=mx.init.Uniform(0.1))\n",
    "        self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout, input_size=num_embed)\n",
    "\n",
    "        self.sampled_decoder = model.ISDense(vocab_size, num_sampled, num_hidden)\n",
    "        self.decoder = nn.Dense(vocab_size, in_units=num_hidden,\n",
    "                                params=self.sampled_decoder.params)\n",
    "\n",
    "    def forward(self, inputs, hidden, sample_mode, *args):\n",
    "        emb = self.drop(self.encoder(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output).reshape((-1, self.num_hidden))\n",
    "        if sample_mode:\n",
    "            sampled_values, targets = args\n",
    "            targets = targets.reshape((-1, 1))\n",
    "            decoded, new_targets = self.sampled_decoder(output, sampled_values, targets)\n",
    "            return decoded, hidden, new_targets\n",
    "        else:\n",
    "            decoded = self.decoder(output)\n",
    "            return decoded, hidden\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "pseudo code\n",
    "```\n",
    "# samples pred\n",
    "pred_sampled = F.FullyConnected(x, weight=w_sampled, bias=b_sampled)\n",
    "# true pred\n",
    "pred_true = (w_true * x).sum(axis=1)\n",
    "# subtract log(q)\n",
    "pred_true = F.broadcast_sub(pred_true, F.log(expected_count_true))\n",
    "pred_sampled = F.broadcast_sub(pred_sampled, F.log(expected_count_sampled))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Set environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Please set `use_gpu` to False if no GPUs are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "context = mx.gpu() if use_gpu else mx.cpu()\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Set hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Load dataset, extract vocabulary, numericalize, and batchify for truncated BPTT\n",
    "\n",
    "For demonstration purpose, we load the validation set for training, and test set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=13777, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "train_dataset = data.WikiText2(segment='val', bos=None, eos='<eos>')\n",
    "val_dataset = data.WikiText2(segment='test', bos=None, eos='<eos>')\n",
    "\n",
    "vocab = nlp.Vocab(data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "    vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data = [\n",
    "    bptt_batchify(x) for x in [train_dataset, val_dataset]\n",
    "]\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Define parameters for model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "num_sampled = 8192\n",
    "num_hidden = 200\n",
    "num_embed = 200\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Create candidate sampler\n",
    "\n",
    "The `LogUniformSampler` class samples classes based on the approximate log uniform or Zipfian distribution <sup>[4]</sup>. \n",
    "\n",
    "P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This sampler is useful when the true classes approximately follow such a distribution like word tokens.\n",
    "\n",
    "Besides `LogUniformSampler`, Gluon NLP also provides other samplers such as `UnigramSampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "candidate_sampler = LogUniformSampler(vocab_size, num_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Create model and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "net = RNNModel(vocab_size, num_embed, num_hidden, num_layers, num_sampled, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (encoder): Embedding(13777 -> 200, float32)\n",
      "  (rnn): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.5)\n",
      "  (sampled_decoder): ISDense(200 -> 13777, with 8192 samples)\n",
      "  (decoder): Dense(200 -> 13777, linear)\n",
      "  (drop): Dropout(p = 0.5, axes=())\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net.initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "parameters = net.collect_params().values()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluation with full softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(net, loss, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = net.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (batch_data, batch_target) in enumerate(data_source):\n",
    "        batch_data = batch_data.as_in_context(ctx)\n",
    "        batch_target = batch_target.as_in_context(ctx)\n",
    "        # evaluate full logits for testing\n",
    "        sample_mode = False\n",
    "        output, hidden = net(batch_data, hidden, sample_mode)\n",
    "        # hidden = detach(hidden)\n",
    "        L = loss(output, batch_target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now that everything is ready, we can start training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_LOG = '[Epoch %d Batch %d/%d] training loss %.2f, ppl %.2f'\n",
    "TRAIN_EPOCH_LOG = '[Epoch %d] Evaluation time cost %.2fs, valid loss %.2f, valid ppl %.2f'\n",
    "TOTAL_LOG = 'Total training time cost %.2f s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training with importance sampled softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, loss, train_data, val_data, epochs, lr, candidate_sampler):\n",
    "    start_train_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        hiddens = net.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=context)\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            with autograd.record():\n",
    "                X, y = data.as_in_context(context), target.as_in_context(context)\n",
    "                sample_mode = True\n",
    "                # pass sampled candidates for training\n",
    "                sampled_values = candidate_sampler(y)\n",
    "                output, hiddens, new_target = net(X, hiddens, sample_mode, sampled_values, y)\n",
    "                batch_L = loss(output, new_target)\n",
    "                L = L + batch_L / X.size\n",
    "            L.backward()\n",
    "            grads = [p.grad(context) for p in parameters]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "            trainer.step(1)\n",
    "            total_L += L.sum().asscalar()\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print(TRAIN_BATCH_LOG%(epoch, i, len(train_data), cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "        mx.nd.waitall()\n",
    "        val_L = evaluate(net, loss, val_data, batch_size, context)\n",
    "        print(TRAIN_EPOCH_LOG%(epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "    print(TOTAL_LOG%(time.time() - start_train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 50/309] training loss 7.80, ppl 2434.99\n",
      "[Epoch 0 Batch 100/309] training loss 6.94, ppl 1029.57\n",
      "[Epoch 0 Batch 150/309] training loss 6.71, ppl 819.89\n",
      "[Epoch 0 Batch 200/309] training loss 6.54, ppl 691.22\n",
      "[Epoch 0 Batch 250/309] training loss 6.42, ppl 612.16\n",
      "[Epoch 0 Batch 300/309] training loss 6.30, ppl 547.02\n",
      "[Epoch 0] Evaluation time cost 28.31s, valid loss 5.99, valid ppl 400.87\n",
      "[Epoch 1 Batch 50/309] training loss 6.36, ppl 577.76\n",
      "[Epoch 1 Batch 100/309] training loss 6.21, ppl 498.41\n",
      "[Epoch 1 Batch 150/309] training loss 6.08, ppl 436.84\n",
      "[Epoch 1 Batch 200/309] training loss 6.05, ppl 422.54\n",
      "[Epoch 1 Batch 250/309] training loss 5.96, ppl 389.14\n",
      "[Epoch 1 Batch 300/309] training loss 5.89, ppl 362.33\n",
      "[Epoch 1] Evaluation time cost 28.10s, valid loss 5.62, valid ppl 275.18\n",
      "[Epoch 2 Batch 50/309] training loss 5.98, ppl 395.63\n",
      "[Epoch 2 Batch 100/309] training loss 5.88, ppl 357.46\n",
      "[Epoch 2 Batch 150/309] training loss 5.76, ppl 317.76\n",
      "[Epoch 2 Batch 200/309] training loss 5.76, ppl 316.96\n",
      "[Epoch 2 Batch 250/309] training loss 5.68, ppl 293.60\n",
      "[Epoch 2 Batch 300/309] training loss 5.63, ppl 279.41\n",
      "[Epoch 2] Evaluation time cost 28.48s, valid loss 5.48, valid ppl 240.63\n",
      "Total training time cost 84.90 s\n"
     ]
    }
   ],
   "source": [
    "train(net, loss, train_data, val_data, epochs, lr, candidate_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practice\n",
    "\n",
    "Change `num_sampled`. How does it affect training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we learnt how to build a language model with softmax approximation via importance sampling.\n",
    "\n",
    "## Reference\n",
    "\n",
    "[1] Chelba, Ciprian, et al. \"One billion word benchmark for measuring progress in statistical language modeling.\" arXiv preprint arXiv:1312.3005 (2013).\n",
    "\n",
    "[2] Bengio, Yoshua, and Jean-Sébastien Senécal. \"Adaptive importance sampling to accelerate training of a neural probabilistic language model.\" IEEE Transactions on Neural Networks 19.4 (2008): 713-722.\n",
    "\n",
    "[3] Jozefowicz, Rafal, et al. \"Exploring the limits of language modeling.\" arXiv preprint arXiv:1602.02410 (2016).\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Zipf%27s_law"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
