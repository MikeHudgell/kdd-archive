{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sklearn.ensemble\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the featurized data selected by active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column_name = 'flagged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('training_set_01.csv').drop('rev_id', axis=1)\n",
    "\n",
    "X_train = training_set.loc[:, :'V511']\n",
    "\n",
    "y_train = training_set.loc[:, label_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('test_set_01.csv').drop('rev_id', axis=1)\n",
    "\n",
    "X_test = test_set.loc[:, :'V511']\n",
    "\n",
    "y_test = test_set.loc[:, label_column_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train default (un-tuned) scikit-learn random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.4 ms, sys: 132 Âµs, total: 63.5 ms\n",
      "Wall time: 62 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifier_model = sklearn.ensemble.RandomForestClassifier(random_state=1)\n",
    "# classifier_model = sklearn.ensemble.RandomForestClassifier(n_estimators=101, max_depth=8, n_jobs=-1, random_state=1)\n",
    "# classifier_model = sklearn.ensemble.RandomForestClassifier(n_estimators=500, criterion='entropy', random_state=1)\n",
    "\n",
    "fitted_model = classifier_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8286844255777241\n"
     ]
    }
   ],
   "source": [
    "pred_prob = fitted_model.predict_proba(X_test)\n",
    "\n",
    "scores = pred_prob[:,1]\n",
    "\n",
    "auc = roc_auc_score(y_test, scores)\n",
    "\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning of PySpark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only needed if not running in a Pyspark kernel\n",
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "            .config(\"spark.jars.packages\", \"Azure:mmlspark:0.13\") \\\n",
    "            .getOrCreate()\n",
    "import mmlspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import mmlspark\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, StructType, StructField\n",
    "\n",
    "import os, urllib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark dataframes\n",
    "\n",
    "tune = spark.createDataFrame(training_set)\n",
    "test = spark.createDataFrame(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune logistic regression, random forest, and GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import TuneHyperparameters\n",
    "from mmlspark.TrainClassifier import TrainClassifier\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "\n",
    "# Define the models to try: Logistic Regression, Random Forest, and Gradient Boosted Trees\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "randForest = RandomForestClassifier()\n",
    "gbt = GBTClassifier()\n",
    "\n",
    "smlmodels = [logReg, randForest, gbt]\n",
    "\n",
    "mmlmodels = [TrainClassifier(model=model, labelCol=label_column_name) for model in smlmodels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark import HyperparamBuilder\n",
    "from mmlspark import RangeHyperParam\n",
    "from mmlspark import DiscreteHyperParam\n",
    "from mmlspark import RandomSpace\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "\n",
    "paramBuilder = \\\n",
    "  HyperparamBuilder() \\\n",
    "    .addHyperparam(logReg, logReg.regParam, RangeHyperParam(0.1, 0.3, isDouble=True)) \\\n",
    "    .addHyperparam(randForest, randForest.numTrees, RangeHyperParam(50, 1000)) \\\n",
    "    .addHyperparam(randForest, randForest.maxDepth, RangeHyperParam(3, 30)) \\\n",
    "    .addHyperparam(randForest, randForest.maxBins, RangeHyperParam(100, 1000)) \\\n",
    "    .addHyperparam(randForest, randForest.impurity, DiscreteHyperParam(['gini', 'entropy'])) \\\n",
    "    .addHyperparam(gbt, gbt.maxBins, RangeHyperParam(100, 1000)) \\\n",
    "    .addHyperparam(gbt, gbt.maxDepth, RangeHyperParam(3, 30))\n",
    "\n",
    "randomSpace = RandomSpace(paramBuilder.build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 537 ms, sys: 295 ms, total: 833 ms\n",
      "Wall time: 15min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bestModel = TuneHyperparameters(\n",
    "              evaluationMetric=\"AUC\", models=mmlmodels, numFolds=3,\n",
    "              numRuns=len(mmlmodels) * 1, parallelism=1,\n",
    "              paramSpace=randomSpace.space(), seed=0).fit(tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: 2\n",
      "elasticNetParam: 0.0\n",
      "family: auto\n",
      "featuresCol: TrainClassifier_4408b3941f51ab92cc84_features\n",
      "fitIntercept: true\n",
      "labelCol: flagged\n",
      "maxIter: 100\n",
      "predictionCol: prediction\n",
      "probabilityCol: probability\n",
      "rawPredictionCol: rawPrediction\n",
      "regParam: 0.2461935574753314\n",
      "standardization: true\n",
      "threshold: 0.5\n",
      "tol: 1.0E-6\n"
     ]
    }
   ],
   "source": [
    "# Print the parameters of the best model\n",
    "\n",
    "bestModelInfo = bestModel._java_obj.getBestModelInfo()\n",
    "\n",
    "for entry in bestModelInfo.split(', '):\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the best model on the held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>evaluation_type</th>\n",
       "      <td>Classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_class_as_0.0_actual_is_0.0</th>\n",
       "      <td>8454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_class_as_0.0_actual_is_1.0</th>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_class_as_1.0_actual_is_0.0</th>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predicted_class_as_1.0_actual_is_1.0</th>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.9274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.801564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.610573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.929847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    \n",
       "evaluation_type                       Classification\n",
       "predicted_class_as_0.0_actual_is_0.0            8454\n",
       "predicted_class_as_0.0_actual_is_1.0             523\n",
       "predicted_class_as_1.0_actual_is_0.0             203\n",
       "predicted_class_as_1.0_actual_is_1.0             820\n",
       "accuracy                                      0.9274\n",
       "precision                                   0.801564\n",
       "recall                                      0.610573\n",
       "AUC                                         0.929847"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmlspark import ComputeModelStatistics\n",
    "\n",
    "prediction = bestModel.transform(test)\n",
    "\n",
    "metrics = ComputeModelStatistics().transform(prediction)\n",
    "\n",
    "metrics.toPandas().transpose().rename(columns={0: ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
