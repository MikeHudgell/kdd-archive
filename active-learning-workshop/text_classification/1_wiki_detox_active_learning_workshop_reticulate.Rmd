---
title: "Active Learning for Text Classification"
output: html_document
params:
  seed: 3                             # seed for random number generator
  initial_examples_per_class: 20      # number of cases from the labeled dataset used to train the initial model
  examples_to_label_per_iteration: 10 # number of cases to label and add to training set per iteration
  num_iterations: 20                  # number of iterations of active learning
  presample_size: 20000               # score and cluster only this many cases per iteration
  monte_carlo_samples: 100            # times to repeat random sampling of training cases for estimating p-values
  mu: 0.5
  sigma: 0.1
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, fig.height=7.5)

```

# Classifying Wiki Detox Comments

Here we use a simple active learning approach for building a text classifier.

```{r load_libraries_and_data}
### Libraries ###
library(dplyr)
library(ggplot2)
library(tidyr)
library(pROC)

source("active_learning_lib_reticulate.R")

FEATURIZED_DATA_FILE <- "attacks_use_encoded.Rds" 
RANDOM_TS_PERFORMANCE_FILE <- "passive_learning_curves_data.Rds"
TEST_SET_SIZE <- 10000

if (!file.exists(FEATURIZED_DATA_FILE)){
  storage_address <- 'https://altutorialweu.blob.core.windows.net/activelearningdemo/'
  download_url <- paste0(storage_address, FEATURIZED_DATA_FILE)
  download.file(download_url, FEATURIZED_DATA_FILE)
}
```

```{r print_parameters}

# params <- list(seed=1, initial_examples_per_class=20, examples_to_label_per_iteration=20, num_iterations=20, presample_size=20000, monte_carlo_samples=10, mu=0.5, sigma=0.1)
params
```

```{r initialize}
set.seed(1)

FEATURIZED_DATA <- readRDS(FEATURIZED_DATA_FILE)
FEATURIZED_DATA <- FEATURIZED_DATA[complete.cases(FEATURIZED_DATA),]

test_set_ids <- sample(FEATURIZED_DATA$rev_id, TEST_SET_SIZE)
TEST_SET <- FEATURIZED_DATA %>% filter(rev_id %in% test_set_ids)

unlabeled_data_df <- FEATURIZED_DATA[!(FEATURIZED_DATA$rev_id %in% TEST_SET$rev_id),]

inputs <- grep("^V", names(FEATURIZED_DATA), value=TRUE)
outcome <- "flagged"
FORM <- paste0(outcome, ' ~ ', paste(inputs, collapse="+"), " - 1")

```


### Split labeled data into training and test sets

```{r split_train_and_test_sets}
set.seed(params$seed)

initial_training_set <- FEATURIZED_DATA %>%
  group_by(flagged) %>%
  do(sample_n(., params$initial_examples_per_class)) %>%
  ungroup %>%
  as.data.frame

table(initial_training_set$flagged)

table(TEST_SET$flagged)
table(TEST_SET$flagged)/nrow(TEST_SET)
```
## Case selection function

```{r select_cases}
get_uncertainty_function <- function(pred_df){
  function(x){
    dnorm(x, mean=params$mu, sd=params$sigma)
  }
}

select_cases <- function(current_results, available_cases, N=params$examples_to_label_per_iteration, presample_size=params$presample_size){
    model <- current_results$model
    presample_size <- min(nrow(available_cases), presample_size)
    candidate_cases <- available_cases[sample(1:nrow(available_cases), presample_size),]
    
    # votes_vec <- predict(model, candidate_cases, type='vote')[,'TRUE']'
    X_candidates <- model.matrix(formula(FORM), candidate_cases)
    votes_vec <- model$predict_proba(X_candidates)[,2]
    predictions_df <- data.frame(rev_id=candidate_cases$rev_id,
                                  flagged=candidate_cases$flagged,
                                  predicted=votes_vec > 0.5,
                                  estimated_probability=votes_vec)

    uncertainty <- get_uncertainty_function(current_results$test_predictions)

    p <- predictions_df$estimated_probability
    u <- uncertainty(p)
    s <- sample(predictions_df$rev_id, N, prob=u, replace=FALSE)
    selected <- predictions_df %>% filter(rev_id %in% s)

    return(selected)
}

```

## Initial model

First we build a model on the small number of examples in the initial training set, and test on the test data.

### Fit model to initial training set

```{r train__initial_model}

initial_model_results <- fit_and_evaluate_model(initial_training_set)
initial_model_results$selected <- select_cases(initial_model_results, unlabeled_data_df)

```

### Results for initial model

#### Confusion matrix

```{r initial_model_confusion}

initial_model_results$confusion

```

#### Performance summary

```{r initial_model_performance}
initial_model_results$performance

```


## Iterate modelling, case selection, and (pseudo) labelling

```{r iterate}

new_sample <- initial_model_results$selected %>% get_new_pseudolabeled_sample(unlabeled_data_df)

current_training_set <- rbind(initial_training_set, new_sample[names(initial_training_set)])

ALREADY_EVALUATED <- initial_model_results$selected$rev_id

iteration_results <- lapply(1:params$num_iterations, function(i){
  results <- fit_and_evaluate_model(current_training_set)
  
  candidate_cases <- unlabeled_data_df[(unlabeled_data_df$rev_id %in% setdiff(unlabeled_data_df$rev_id,
                                                                                ALREADY_EVALUATED)),]
  results$selected <- select_cases(results, candidate_cases)

  ALREADY_EVALUATED <<- c(ALREADY_EVALUATED, results$selected$rev_id)

  next_sample <- results$selected %>% get_new_pseudolabeled_sample(unlabeled_data_df)
  
  current_training_set <<- rbind(current_training_set, next_sample[names(current_training_set)])

  results
})
```

```{r save_selected_training_set, eval=FALSE}
# current_training_set also includes the ones selected on the last round
not_run_yet <- iteration_results[[length(iteration_results)]]$selected$rev_id
training_set <- current_training_set[!(current_training_set$rev_id %in% not_run_yet),]

write.csv(training_set, sprintf("training_set_%02d.csv", params$seed), row.names=FALSE, quote=FALSE)
write.csv(TEST_SET, sprintf("test_set_%02d.csv", params$seed), row.names=FALSE, quote=FALSE)

```

This shows the change in the metrics, with each row showing an iteration.

```{r visualize_metrics_by_iteration}
iteration_performance <- do.call("rbind", lapply(iteration_results, function(ires) ires$performance))

(performance_table <- rbind(initial_model_results$performance, iteration_performance))
```


## Final model results
### Confusion Matrix

```{r final_model}
final_model_results <- iteration_results[[params$num_iterations]]
final_model_results$confusion
```

### Performance summary

Summary of performance using cases selected with active learning:

```{r summary_of_preformance_using_selected_cases}

(selected_sample_results <- final_model_results$performance)
```

### Visualizing improvement for actively learned model

This series of ROC curves shows how performance changes with iterations of active learning.

```{r visualizing_improvement, eval=TRUE}
plot_roc_history(initial_model_results, iteration_results)

```

### Comparing learning curves of active and passive learning

```{r active_vs_passive_learning_curves}
get_random_training_set_performance <- function(ts_sizes){
  random_training_set <- initial_training_set

  random_training_set_results <- lapply(c(0,diff(ts_sizes)), function(tss){
    new_ids <- sample(setdiff(unlabeled_data_df$rev_id, random_training_set$rev_id), tss)
    new_cases <- unlabeled_data_df %>% filter(rev_id %in% new_ids)
    random_training_set <<- rbind(random_training_set, new_cases)
    fit_and_evaluate_model(random_training_set)
  })

  random_ts_performance <- random_training_set_results %>%
    lapply("[[", "performance") %>%
    do.call(bind_rows, .)

  random_ts_performance$sample_selection_mode <- "random"

  random_ts_performance
}

performance_table <- as.data.frame(performance_table)
ts_sizes <- performance_table$tss

if (file.exists(RANDOM_TS_PERFORMANCE_FILE)){
  random_ts_performance_list <- readRDS(RANDOM_TS_PERFORMANCE_FILE)
} else {
  NUM_TSS_COMPARISONS <- 3
  random_ts_performance_list <- lapply(1:NUM_TSS_COMPARISONS, function(i){
    rtsp <- get_random_training_set_performance(ts_sizes)
    rtsp$group <- i
    rtsp
  })

  saveRDS(random_ts_performance_list, RANDOM_TS_PERFORMANCE_FILE)
}

performance_table$sample_selection_mode <- "active"
performance_table$group <- 0

performance_data <- bind_rows(random_ts_performance_list, performance_table)
names(performance_data)[5] <- "run"

performance_data %>% 
  gather(key="metric", value="value", -tss, -sample_selection_mode, -run) %>% 
  ggplot(aes(x=log10(tss), y=value, col=sample_selection_mode, group=run)) + 
    geom_line(size=1, alpha=0.5) + 
    facet_grid(metric ~ ., scales="free")

```


## Monte Carlo Estimation of P-values

What is the probability that a set of randomly chosen cases would improve the performance of the model as much as the selected cases did? We'll add the same number of examples to the training set, except that now they will be randomly chosen. We'll repeat this sampling, training, and evaluation process `r params$monte_carlo_samples` times, and see how many of those times we beat the performance of the selected cases.


```{r bootstrap_probability}

(N <- iteration_results[[params$num_iterations]]$performance[['tss']] - nrow(initial_training_set))

available_cases <- unlabeled_data_df

random_sample_results <- sapply(1:params$monte_carlo_samples, function(i){
  new_sample <- available_cases[sample(1:nrow(available_cases), N, replace=FALSE),]

  training_set_new <- rbind(initial_training_set, new_sample[names(initial_training_set)])

  fit_and_evaluate_model(training_set_new)$performance
})

```

### P-values

This table shows the number of times out of `r params$monte_carlo_samples` tries that the randomly selected cases equalled or exceeded the performance of the actively learned cases for each metric. These numbers are estimated P-values in percent.

```{r p_values}
mapply ( 
  function(metric) sum(random_sample_results[metric,] >= selected_sample_results[[metric]]), 
  row.names(random_sample_results)
) / params$monte_carlo_samples


```

## Model trained with all available "unlabeled" cases

For comparison, we'll build a model as though we had gone through and labeled all `r nrow(available_cases)` of the usable new examples.

```{r fmr_file}
FULL_MODEL_RESULTS_FILE <- "full_model_results.Rds"
```
```{r full_model_results}

training_set_full <- rbind(initial_training_set, available_cases[names(initial_training_set)])

if (file.exists(FULL_MODEL_RESULTS_FILE)){
  full_model_results <- readRDS(FULL_MODEL_RESULTS_FILE)
} else {
  full_model_results <- fit_and_evaluate_model(training_set_full)
  saveRDS(full_model_results, FULL_MODEL_RESULTS_FILE)
}

full_model_results$confusion

full_model_results$performance

plot_roc_history(initial_model_results, list(final_model_results, full_model_results))

```


## Compare initial model to final model predictions [[NEW]]

```{r compare_initial_to_final}

final_res <- final_model_results$test_predictions[c("rev_id", "estimated_probability")]
names(final_res)[2] <- 'final_score'

full_res <- full_model_results$test_predictions[c("rev_id", "estimated_probability")]
names(full_res)[2] <- 'full_score'

                                                  
pred_frame <- initial_model_results$test_predictions %>% 
  mutate(initial_score = estimated_probability) %>%
  select(rev_id, flagged, initial_score) %>%
  inner_join(final_res, by="rev_id") %>%
  inner_join(full_res, by="rev_id")
  
pred_frame %>% ggplot(aes(x=initial_score, y=final_score, col=flagged)) + geom_point()

pred_frame %>% ggplot(aes(x=initial_score, fill=flagged)) + 
  geom_density(alpha=0.5) + ggtitle("distribution of initial scores")

pred_frame %>% ggplot(aes(x=final_score, fill=flagged)) + 
  geom_density(alpha=0.5) + ggtitle("distribution of final scores")
 
pred_frame %>% ggplot(aes(x=full_score, fill=flagged)) + 
  geom_density(alpha=0.5) + ggtitle("distribution of full-model scores")
 
```

Let's look at some examples of comments that are false negatives under the final model.
```{r misclassified_examples, eval=FALSE}
text_data <- read.csv("text_data.csv", stringsAsFactors=FALSE) # actually, we only need the text from the test set...

text_data$text <- text_data$text %>% 
  gsub("NEWLINE_TOKEN", "\n", .) %>% 
  gsub("\n+", "\n", .) %>%
  gsub(" +", " ", .)

false_negatives <- with(final_model_results$test_predictions, 
                        rev_id[(estimated_probability < 0.1) & flagged])

false_neg_data <- text_data[text_data$rev_id %in% false_negatives, ]

false_neg_data$text[1:5]
```

# TO DO:

  * Featurize with an embedding that contains more dirty words (e.g., Reddit NSFW).
  * `estimated_probability` is really just a score in the range between 0 and 1.
  * Use a smarter uncertainty function; target utility.

