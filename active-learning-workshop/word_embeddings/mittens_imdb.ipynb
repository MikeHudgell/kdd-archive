{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Glove with Mittens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "This notebook needs the mittens package, to install:\n",
    "\n",
    "`pip install --user mittens`\n",
    "\n",
    "#### Notes\n",
    "\n",
    "1. To limit words using the `mincount` parameter in the `build_weighted_matrix` function (default is likely too high for small corpora: 300)\n",
    "1. Currently only doing a sample for timing. Undo the slice before `build_weighted_matrix` call\n",
    "1. The usage of the `NltkPreprocessor` and `NltkTokenizer` can be replaced with a different one. There's no major dependency on `tatk` here\n",
    "1. To use the `tf` implementation, set `USE_TF` to `True`\n",
    "1. The trials aren't for iterative training\n",
    "1. You can also use the `GloVe` implementation in `mittens`, shown in the last cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlabeled IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 18436180092534167647, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 15863893197\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 11333334022390198729\n",
       " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 7712:00:00.0, compute capability: 6.0\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, \n",
    "    confusion_matrix, f1_score)\n",
    "\n",
    "USE_TF = True\n",
    "\n",
    "if USE_TF:\n",
    "    from mittens.tf_mittens import Mittens, GloVe # for tensorflow implementation\n",
    "else:\n",
    "    from mittens.np_mittens import Mittens, GloVe # for vectorized numpy\n",
    "\n",
    "data_path = pathlib.Path.home() / \"tatk\" / \"resources\" / \"data\" / \"imdb\" / \"aclImdb\" / \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/alizaidi/tatk/resources/data/imdb/aclImdb/train/unsupBow.feat'),\n",
       " PosixPath('/home/alizaidi/tatk/resources/data/imdb/aclImdb/train/urls_pos.txt'),\n",
       " PosixPath('/home/alizaidi/tatk/resources/data/imdb/aclImdb/train/unsup'),\n",
       " PosixPath('/home/alizaidi/tatk/resources/data/imdb/aclImdb/train/urls_unsup.txt'),\n",
       " PosixPath('/home/alizaidi/tatk/resources/data/imdb/aclImdb/train/labeledBow.feat'),\n",
       " PosixPath('/home/alizaidi/tatk/resources/data/imdb/aclImdb/train/pos'),\n",
       " PosixPath('/home/alizaidi/tatk/resources/data/imdb/aclImdb/train/neg'),\n",
       " PosixPath('/home/alizaidi/tatk/resources/data/imdb/aclImdb/train/urls_neg.txt')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_path.glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_directory(directory_path,\n",
    "                        dirs_to_check, \n",
    "                        column_names):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with a single row for each txt file in `directory_path`\n",
    "    \"\"\"\n",
    "    \n",
    "    import pathlib\n",
    "    import pandas as pd\n",
    "    \n",
    "    source_path = pathlib.Path(directory_path)\n",
    "    list_dirs = [x for x in source_path.iterdir()]\n",
    "    match_dirs = [x for x in list_dirs if any(xs in str(x) for xs in dirs_to_check)]\n",
    "    files = sum([list(match.glob(\"*.txt\")) for match in match_dirs], [])\n",
    "    df = pd.concat([pd.read_csv(x, header=None, sep = \"\\n\") for x in files])\n",
    "    df.columns = column_names\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_df = read_from_directory(str(data_path), dirs_to_check=\"unsup\", column_names=[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Makers of erotic thrillers need to be careful,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A ghoulish mixture of Liszt, murder, violence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is severely underrated and was very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apart from the wooden acting, the heavy-handed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Having read much of the criticism and praise o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  Makers of erotic thrillers need to be careful,...\n",
       "0  A ghoulish mixture of Liszt, murder, violence ...\n",
       "0  This movie is severely underrated and was very...\n",
       "0  Apart from the wooden acting, the heavy-handed...\n",
       "0  Having read much of the criticism and praise o..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_subset = imdb_df.sample(n=10**3)\n",
    "imdb_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = read_from_directory(str(data_path), dirs_to_check=[\"pos\", \"neg\"], column_names=[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tatk.preprocessing.nltk_preprocessor import NltkPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 2018-05-17 17:15:11,303 INFO azureml.text:machine info {\"os_type\": \"Linux\", \"is_dsvm\": true} \n",
      "F1 2018-05-17 17:15:11,306 INFO azureml.text:8481512779234386476 NltkPreprocessor::tatk_transform ==> start \n",
      "NltkPreprocessor::tatk_transform ==> start\n",
      "F1 2018-05-17 17:15:30,764 INFO azureml.text:8481512779234386476 NltkPreprocessor::tatk_transform ==> end \t Time taken: 0.32 mins \n",
      "NltkPreprocessor::tatk_transform ==> end \t Time taken: 0.32 mins\n"
     ]
    }
   ],
   "source": [
    "preprocessor = NltkPreprocessor(input_col='review', output_col='tokens')\n",
    "imdb_df = preprocessor.tatk_transform(imdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_text = imdb_df[preprocessor.get_output_col()].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Makers of erotic thrillers need to be careful , as that is a genre that , if not handled carefully , can quickly fall prey to silliness and excess ( think \" Fatal Attraction \"). \" Swimming Pool \" is a thriller in the style of \" The Deep End ,\" and more than once I was struck by similarities between the two in their respective tones and reliance on water as a recurring visual motif . Also , both films have a middle - aged female as the protagonist who becomes involved in covering up for the actions of a child ( in \" The Deep End \" a literal child , in \" Swimming Pool \" a figurative one ). Also , both films are completely unpredictable . Neither goes the direction in which the viewer thinks it \\' s going to . However , \" Swimming Pool \" is much more abstract , and its ending leaves you wanting to watch the whole thing over immediately with an entirely different perspective on the action . This gimmick always makes for a memorable ending in movies that employ it , but too often it makes the rest of the movie seem somewhat pale in comparison , and this is the case here . \" Swimming Pool \" plays tricks with your perceptions , but the finale to which the film builds seems somewhat anti - climactic when it finally comes . < br />< br /> It \\' s a leisurely paced film , and you \\' ll need to have patience with it . You \\' ll also need to have patience with the main character , played by Charlotte Rampling . Rampling gives a fine performance , but her character is really unlikable ( intentionally so ), and it \\' s always a liability for any story that focuses almost solely on one person to make that person unlikable , or at least sympathetic .< br />< br />\" Swimming Pool ,\" though billed as an erotic thriller , is really about the creative process ( I think ), and I won \\' t say anymore about that because to do so will give away the ending . It \\' s an interesting idea , imperfectly executed .< br />< br /> Grade : B'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Weight Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhraseDetector::tatk_fit_transform ==> start\n",
      "PhraseDetector::tatk_fit_transform ==> end \t Time taken: 7.23 mins\n"
     ]
    }
   ],
   "source": [
    "from tatk.preprocessing.phrase_detector import PhraseDetector\n",
    "phrase_detect = PhraseDetector(input_col=\"tokens\", output_col=\"phrase_tokens\")\n",
    "imdb_phrase_df = phrase_detect.tatk_fit_transform(imdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Makers of erotic thrillers need to be careful , as that is a genre that , if not handled carefully , can quickly fall_prey to silliness and excess ( think \" Fatal_Attraction \"). \" Swimming_Pool \" is a thriller in the style of \" The Deep End ,\" and more_than once I was struck_by similarities_between the two in their_respective tones and reliance_on water as a recurring visual motif . Also , both films have a middle -_aged female as the protagonist who becomes_involved in covering up for the actions of a child ( in \" The Deep End \" a literal child , in \" Swimming_Pool \" a figurative one ). Also , both films are completely unpredictable . Neither goes the direction in which the viewer thinks it \\'_s going to . However , \" Swimming_Pool \" is much_more abstract , and its ending leaves you wanting to watch the whole_thing over immediately with an entirely_different perspective on the action . This gimmick always makes for a memorable ending in movies that employ it , but too_often it makes the rest of the movie seem somewhat pale in comparison , and this is the case here . \" Swimming_Pool \" plays tricks with your perceptions , but the finale to which the film builds seems somewhat anti_- climactic when it finally comes . <_br /><_br /> It \\'_s a leisurely paced film , and you \\'_ll need to have patience with it . You \\'_ll also need to have patience with the main_character , played_by Charlotte_Rampling . Rampling gives a fine_performance , but her character is really unlikable ( intentionally so ), and it \\'_s always a liability for any story that focuses almost solely_on one person to make that person unlikable , or at_least sympathetic .<_br /><_br />\" Swimming_Pool ,\" though billed_as an erotic_thriller , is really about the creative_process ( I think ), and I won \\'_t say anymore about that because to do so will give_away the ending . It \\'_s an_interesting idea , imperfectly executed .<_br /><_br /> Grade_: B',\n",
       "       \"A ghoulish mixture of Liszt , murder , violence and carrots , ' Rhapsody Rabbit ' is an exuberantly inventive Bugs_Bunny cartoon which manages to explode the boundaries of its single setting . Bugs is a famed pianist , the kind of fastidious virtuoso you still find today , but worshipped in the 40s because arrogant eccentricity somehow signalled class . Having removed his many gloves , Bugs , a proto - Glenn Gould seats himself down in near - religious preparation , only to be interrupted_by two loud coughs . He shoots the culpable party .<_br /><_br /> The film is full of gloriously unpredictable moments like this , helping it transcend the immediate object of satire , which has dated , now that Hollywood has given up as unprofitable the attempt to educate audiences in high culture . So Bugs interrupts his playing to chomp on a carrot , or play with his feet . One lovely sequence has him gathering all the keys and throwing them back in perfect rhythm . Like Fischinger '_s ' Allegretto ', ' Rhapsody ' is animated music , full of a strange , mercurial , yet elegant fluidity .<_br /><_br /> The centrepiece is a Tom - and - Jerry - like battle_between Bugs and a small mouse who tries to undermine Bugs ' pretensions , changing the solemn rhapsody for swing at one point . Despite the violence and disruption , conflict , as so often in music , leads not to chaos , but harmonic rapture . Freleng is no Tex_Avery - his use of colour and camerawork is restrained - but the relative plausibility of his composition have a pleasure all of their_own .\",\n",
       "       \"This movie is severely underrated and was very badly marketed as a family comedy . This is a drama about trying to fit in , discovering reasons for living , true_love and the human_condition . Sure it has its funny scenes but it '_s incidental to the rest . The characters , the robot in particular , are profoundly likable ( ok , with a few_exceptions ;) and I won '_t mention how or why , but it had truly beautiful moments , both small and big . One of the movies where I identified the most with the hero and shed the most tears throughout and making me cry is very hard to do . I might love this movie for the emotional_impact but I was fascinated_by several other aspects including the robotics and the passage_of_time . Robin_Williams was great in this role .<_br /><_br />_Rating : 9 out_of_10\",\n",
       "       \"Apart_from the wooden_acting , the heavy -_handed direction , the vapid dialogue , farcical ' action ' scenes and the incredulous plot , this movie was the worst film I '_ve seen in a long , long_time . Given a choice between stabbing myself repeatedly with a pen and watching this movie again , I '_ll take the pen .\"], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_phrase_df.phrase_tokens.values[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TATK Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tatk.preprocessing.nltk_tokenizer import NltkTokenizer\n",
    "basic_tokenizer = NltkTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = imdb_phrase_df.phrase_tokens.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Weighted Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _window_based_iterator(toks, window_size, weighting_function):\n",
    "    for i, w in enumerate(toks):\n",
    "        yield w, w, 1\n",
    "        left = max([0, i-window_size])\n",
    "        for x in range(left, i):\n",
    "            yield w, toks[x],weighting_function(x)\n",
    "        right = min([i+1+window_size, len(toks)])\n",
    "        for x in range(i+1, right):\n",
    "            yield w, toks[x], weighting_function(x)\n",
    "\n",
    "def build_weighted_matrix(corpus, tokenizing_func=basic_tokenizer,\n",
    "        mincount=300, vocab_size=None, window_size=10,\n",
    "        weighting_function=lambda x: 1 / (x + 1)):\n",
    "\n",
    "    \"\"\"Builds a count matrix based on a co-occurrence window of\n",
    "    `window_size` elements before and `window_size` elements after the\n",
    "    focal word, where the counts are weighted based on proximity to the\n",
    "    focal word.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : iterable of str\n",
    "        Texts to tokenize.\n",
    "    tokenizing_func : function\n",
    "        Must map strings to lists of strings.\n",
    "    mincount : int\n",
    "        Only words with at least this many tokens will be included.\n",
    "    vocab_size : int or None\n",
    "        If this is an int above 0, then, the top `vocab_size` words\n",
    "        by frequency are included in the matrix, and `mincount`\n",
    "        is ignored.\n",
    "    window_size : int\n",
    "        Size of the window before and after. (So the total window size\n",
    "        is 2 times this value, with the focal word at the center.)\n",
    "    weighting_function : function from ints to floats\n",
    "        How to weight counts based on distance. The default is 1/d\n",
    "        where d is the distance in words.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        This is guaranteed to be a symmetric matrix, because of the\n",
    "        way the counts are collected.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_text = tokenizing_func.transform(corpus)\n",
    "    tokens = [x.split(\" \") for x in corpus]\n",
    "\n",
    "    # Counts for filtering:\n",
    "    wc = defaultdict(int)\n",
    "    for toks in tokens:\n",
    "        for tok in toks:\n",
    "            wc[tok] += 1\n",
    "    if vocab_size:\n",
    "        srt = sorted(wc.items(), key=itemgetter(1), reverse=True)\n",
    "        vocab_set = {w for w, c in srt[: vocab_size]}\n",
    "    else:\n",
    "        vocab_set = {w for w, c in wc.items() if c >= mincount}\n",
    "    vocab = sorted(vocab_set)\n",
    "    n_words = len(vocab)\n",
    "\n",
    "    # Weighted counts:\n",
    "    counts = defaultdict(float)\n",
    "    for toks in tokens:\n",
    "        window_iter = _window_based_iterator(toks, window_size, weighting_function)\n",
    "        for w, w_c, val in window_iter:\n",
    "            if w in vocab_set and w_c in vocab_set:\n",
    "                counts[(w, w_c)] += val\n",
    "\n",
    "    # Matrix:\n",
    "    X = np.zeros((n_words, n_words))\n",
    "    for i, w1 in enumerate(vocab):\n",
    "        for j, w2 in enumerate(vocab):\n",
    "            X[i, j] = counts[(w1, w2)]\n",
    "\n",
    "    # DataFrame:\n",
    "    X = pd.DataFrame(X, columns=vocab, index=pd.Index(vocab))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NltkTokenizer::transform ==> start\n",
      "Time taken: 2.65 mins\n",
      "NltkTokenizer::transform ==> end\n"
     ]
    }
   ],
   "source": [
    "X = build_weighted_matrix(preprocessed_text, vocab_size=10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Mittens with External Glove Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_trials = 5\n",
    "n_trials = 1\n",
    "\n",
    "max_iter = 50000\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "eta = 0.05\n",
    "\n",
    "embedding_path = pathlib.Path(\"/datapascal\") / \"data\" / \"trained_embeddings\" / \"glove\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_glove_lookup(glove_filename):\n",
    "    \"\"\"Turns an external GloVe file into a defaultdict that returns\n",
    "    the learned representation for words in the vocabulary and\n",
    "    random representations for all others.\n",
    "    \"\"\"\n",
    "    glove_lookup = glove2dict(glove_filename)\n",
    "    glove_lookup = defaultdict(lambda : get_random_rep(), glove_lookup)\n",
    "    return glove_lookup\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        data = {line[0]: np.array(list(map(float, line[1: ]))) for line in reader}\n",
    "    return data\n",
    "\n",
    "def create_lookup(X):\n",
    "    \"\"\"Map a dataframe to a lookup that returns random vector reps\n",
    "    for new words, adding them to the lookup when this happens.\n",
    "    \"\"\"\n",
    "    embedding_dim = X.shape[1]\n",
    "    data = defaultdict(lambda : get_random_rep())\n",
    "    for w, vals in X.iterrows():\n",
    "        data[w] = vals.values\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOVE_LOOKUP = create_glove_lookup(str(embedding_path / \"glove.6B.300d.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(train_data, test_data, lookup, label, trial_num):\n",
    "    \"\"\"Run a standard IMDB movie review experiment using `lookup` as \n",
    "    the basis for representing examples. The results are pickled to a \n",
    "    file called \"results/imdb_{label}.pickle\"    \n",
    "    \"\"\"        \n",
    "    output_filename = \"results/imdb_{}_trial{}.pickle\".format(label, trial_num)            \n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # Model:\n",
    "    cv = GridSearchCV(\n",
    "        RandomForestClassifier(), \n",
    "        param_grid={\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'max_depth': [3, 5, None]}, \n",
    "        refit=True, \n",
    "        n_jobs=-1)  \n",
    "    \n",
    "    # Split:\n",
    "    X_train, y_train = featurize(train_data, lookup)\n",
    "    X_test, y_test = featurize(test_data, lookup)\n",
    "    \n",
    "    # Fit with best estimator and predict:\n",
    "    cv.fit(X_train, y_train)\n",
    "    predictions = cv.predict(X_test) \n",
    "    \n",
    "    # CV info:\n",
    "    results['cv_results'] = cv.cv_results_\n",
    "    results['best_params'] = cv.best_params_\n",
    "    results['best_score'] = cv.best_score_\n",
    "        \n",
    "    # Test-set scoring:\n",
    "    acc = accuracy_score(y_test, predictions)               \n",
    "    results['accuracy'] = acc\n",
    "    results['confusion_matrix'] = confusion_matrix(y_test, predictions)\n",
    "    results['f1'] = f1_score(y_test, predictions, average=None)\n",
    "    results['f1_macro'] = f1_score(y_test, predictions, average='macro')\n",
    "    results['f1_micro'] = f1_score(y_test, predictions, average='micro')\n",
    "    \n",
    "    # Summary report:\n",
    "    print(\"Accuracy: {0:0.04%}\".format(acc))\n",
    "    print(\"Best params:\", cv.best_params_)\n",
    "          \n",
    "    # Storage:\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_num in range(1, n_trials+1):\n",
    "    mittens = Mittens(max_iter=max_iter, n=embedding_dim, eta=eta, mittens=1.0)\n",
    "    G_mittens = mittens.fit(\n",
    "        X.values, \n",
    "        vocab=list(X.index), \n",
    "        initial_embedding_dict=GLOVE_LOOKUP)\n",
    "    G_mittens = pd.DataFrame(G_mittens, index=X.index)\n",
    "    G_mittens.to_csv(\"imdb10K_mittens_embedding_{}.csv\".format(trial_num))\n",
    "    mittens_lookup = create_lookup(G_mittens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Glove, no initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_num in range(1, n_trials+1):    \n",
    "    glove = GloVe(max_iter=max_iter, n=embedding_dim, learning_rate=eta)\n",
    "    G = glove.fit(X.values)\n",
    "    G = pd.DataFrame(G, index=X.index)\n",
    "    G.to_csv(\"imdb_glove_embedding_{}.csv\".format(trial_num))\n",
    "    imdb_glove_lookup = create_lookup(G)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aztatk]",
   "language": "python",
   "name": "conda-env-aztatk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
