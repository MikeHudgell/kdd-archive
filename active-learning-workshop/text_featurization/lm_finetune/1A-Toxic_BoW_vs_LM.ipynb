{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurizing Toxic Comments Using Pre-Trained Word Vectors and a Language Model's Encoder\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides an analysis of featurization methods for text. The central idea we examine is that we can represent text (words, phrases, and even entire sentences or paragraphs) as vectors. However, we'll see that some vector representations may provide more semantic information than others. \n",
    "\n",
    "## Pre-Trained Word Vectors\n",
    "\n",
    "As a first foray we featurize our data using publically available pre-trained word vectors. There are numerous word vectors available, the most common being [Word2Vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), and [fasttext](https://github.com/facebookresearch/fastText/). We'll use the **GloVe** vectors trained on Wikipedia and Gigaword 5. \n",
    "\n",
    "Our input dataset contains a variable sequence of tokens (words), which we vectorize into a list of real-valued vectors. In order to use a machine learning model with such a representation we need to transform it into a fixed-vector representation. We can do this by many different aggregation schemes: sum/mean, max, min, etc. For this notebook we simply utilize unweighted averages of all the tokens, but you'll likely find that for some applications it may be more useful to consider max/min in addition, and concatenate multiple representations.\n",
    "\n",
    "\n",
    "![](https://image.slidesharecdn.com/starsem-170916142844/95/yejin-choi-2017-from-naive-physics-to-connotation-modeling-commonsense-in-frame-semantics-83-638.jpg?cb=1505572199)\n",
    "\n",
    "_image credit: Yejin Choi - 2017 - From Naive Physics to Connotation: Modeling Commonsense in Frame Semantics_\n",
    "\n",
    "_quote credit: Ray Mooney_\n",
    "\n",
    "\n",
    "## Language Model Encoders\n",
    "\n",
    "We'll then examine a more advanced method of featurizing our sequence of tokens. In particular, we'll use the encoder from a pre-trained language model. The encoder is a fixed-length vector representation that is typically the last hidden vector in a recurrent neural network trained for machine translation or language modeling.\n",
    "\n",
    "\n",
    "![](http://ruder.io/content/images/2018/07/lm_objective.png)\n",
    "\n",
    "_image credit: Seabstain Ruder and TheGradient: NLP's ImageNet moment has arrived_\n",
    "\n",
    "Our hope is that rather than naively aggregating our word vectors by their average representation, the last hidden layer will contain contextual information from the entire sequence of tokens.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:\n",
    "\n",
    "We import our dataset of comments to Wikipedia page-edits from our helper `load_data`. We'll also import a dictionary of GloVe vectors and a helper function for using it to lookup word vectors for our tokenized comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 70.1MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda/envs/embeddings/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /anaconda/envs/embeddings/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS=4\n",
    "\n",
    "from load_data import load_wiki_attacks, load_attack_encoded\n",
    "from load_data import tokenize, create_glove_lookup, download_glove\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "data_dir = pathlib.Path(\"/data\") / \"active-learning-data\" / \"active-learning-workshop\" / \"text_featurization\" / \"data\"\n",
    "\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [spaCy](https://spacy.io/) library to tokenize our text, but aside from some prior data cleanup there's nothing fancy happening in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% | 822 MB | 7.40 MB/s | 111 sec elapsed"
     ]
    }
   ],
   "source": [
    "glove_src =  str(data_dir / \"glove.6B.300d.txt\")\n",
    "if not pathlib.Path(glove_src).exists():\n",
    "    download_glove(data_dir)\n",
    "glove_lookup = create_glove_lookup(glove_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% | 61 MB | 7.78 MB/s | 7 sec elapsed"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[  , This, is, not, creative, ., Those, are, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[    , the, term, standard, model, is, itself,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[  , True, or, false, ,, the, situation, as, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[Next, ,, maybe, you, could, work, on, being, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[This, page, will, need, disambiguation, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[    , Important, note, for, all, sysops, Ther...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [  , This, is, not, creative, ., Those, are, t...  \n",
       "1  [    , the, term, standard, model, is, itself,...  \n",
       "2  [  , True, or, false, ,, the, situation, as, o...  \n",
       "3  [Next, ,, maybe, you, could, work, on, being, ...  \n",
       "4        [This, page, will, need, disambiguation, .]  \n",
       "5  [    , Important, note, for, all, sysops, Ther...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df = load_wiki_attacks(data_dir)\n",
    "toxic_df = tokenize(toxic_df, \"comment_text\")\n",
    "toxic_df.loc[:5, ['comment_text', \"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with GloVe:\n",
    "\n",
    "We can use our `glove_lookup` dictionary to vectorize all the tokens in our text. We apply the function to every token in our comment, and then take the average over all word vectors. Again, you should definitely consider other aggregation methods such as max/min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df['glove_aggregate'] = toxic_df.tokens.apply(lambda x: np.mean([glove_lookup[v] for v in x], axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>glove_aggregate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[  , This, is, not, creative, ., Those, are, t...</td>\n",
       "      <td>[-0.13136993201600208, 0.04167373661270492, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[    , the, term, standard, model, is, itself,...</td>\n",
       "      <td>[-0.146124050171877, 0.06522088903821076, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[  , True, or, false, ,, the, situation, as, o...</td>\n",
       "      <td>[-0.0017423238006166043, 0.018247330931456952,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[Next, ,, maybe, you, could, work, on, being, ...</td>\n",
       "      <td>[-0.09640093257641184, 0.05173814062729899, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[This, page, will, need, disambiguation, .]</td>\n",
       "      <td>[-0.1910730302010171, -0.018057031742265847, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[    , Important, note, for, all, sysops, Ther...</td>\n",
       "      <td>[-0.14204747110960014, 0.0963638677084288, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [  , This, is, not, creative, ., Those, are, t...   \n",
       "1  [    , the, term, standard, model, is, itself,...   \n",
       "2  [  , True, or, false, ,, the, situation, as, o...   \n",
       "3  [Next, ,, maybe, you, could, work, on, being, ...   \n",
       "4        [This, page, will, need, disambiguation, .]   \n",
       "5  [    , Important, note, for, all, sysops, Ther...   \n",
       "\n",
       "                                     glove_aggregate  \n",
       "0  [-0.13136993201600208, 0.04167373661270492, 0....  \n",
       "1  [-0.146124050171877, 0.06522088903821076, 0.02...  \n",
       "2  [-0.0017423238006166043, 0.018247330931456952,...  \n",
       "3  [-0.09640093257641184, 0.05173814062729899, 0....  \n",
       "4  [-0.1910730302010171, -0.018057031742265847, -...  \n",
       "5  [-0.14204747110960014, 0.0963638677084288, -0....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.loc[:5, [\"comment_text\", \"tokens\", \"glove_aggregate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model \n",
    "\n",
    "Our language model encoder utilizes pre-trained language models hosted on [TensorFlow Hub](https://www.tensorflow.org/hub/modules/text). \n",
    "\n",
    "Our helper script `encoder.py` provides a simple class entitled `encoder` with methods for encoding text using three different encoder models: [ELMO](http://www.aclweb.org/anthology/N18-1202), [USE](https://arxiv.org/pdf/1803.11175.pdf), and [NNLM](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Imports:\n",
    "\n",
    "The class is a bit verbose for readability, but it's conceptually very simple. We load the pre-trained module, which defines a static computational graph with the learned weights from the language model on it's dataset. We initialize this computational graph into a Keras session, which we can then use for fine-tuning or for featurizing an input sequence by computing a forward pass of the computational graph. Note, we could have also just used `tensorflow` directly to do the model building and training, but Keras has some helpful utilities for data min-batching and pre-fetching that makes this very easy (at the cost of some incompatibilities: [issues with fine-tuning may arise](https://groups.google.com/a/tensorflow.org/forum/#!topic/hub/Y4AdAM7HpX0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from encoder import encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "??encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurized Dataset\n",
    "\n",
    "Here's an example usage of converting the `comment_text` into a fixed sequence using our encoder and the **Universal Sentence Encoder**:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "use_encoder = encoder(model=\"use\")\n",
    "featurizer = use_encoder.transform_model()\n",
    "featurizer.summary()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    transformed_review = featurizer.predict(toxic_df.comment_text.values, batch_size=64)\n",
    "```\n",
    "\n",
    "This operation will take some time, ~1.5 hours on a machine with 16 cores. We have a pre-featurized version of this dataset already saved for you, which you can download using our helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% | 694 MB | 7.68 MB/s | 90 sec elapsed"
     ]
    }
   ],
   "source": [
    "encoded_attacks = load_attack_encoded(data_dir)\n",
    "toxic_df['encoded_comment'] = encoded_attacks.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>encoded_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is not creative . Those are the diction...</td>\n",
       "      <td>[0.026781896, -0.05754256, 0.033774074, 0.0097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the term standard model is itself less NPO...</td>\n",
       "      <td>[0.011424046, -0.009576778000000001, -0.026437...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>[0.00041413374, 0.08557465, 0.024096673, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Next, maybe you could work on being less conde...</td>\n",
       "      <td>[0.058229163, 0.048170675, -0.054312646, 0.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>[0.05511004, 0.017830834, -0.08593257, -0.0343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Important note for all sysops There is a b...</td>\n",
       "      <td>[0.07102389, 0.029018747, -0.04301559, -0.0764...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0    This is not creative . Those are the diction...   \n",
       "1      the term standard model is itself less NPO...   \n",
       "2    True or false, the situation as of March 200...   \n",
       "3  Next, maybe you could work on being less conde...   \n",
       "4                This page will need disambiguation.   \n",
       "5      Important note for all sysops There is a b...   \n",
       "\n",
       "                                     encoded_comment  \n",
       "0  [0.026781896, -0.05754256, 0.033774074, 0.0097...  \n",
       "1  [0.011424046, -0.009576778000000001, -0.026437...  \n",
       "2  [0.00041413374, 0.08557465, 0.024096673, -0.09...  \n",
       "3  [0.058229163, 0.048170675, -0.054312646, 0.029...  \n",
       "4  [0.05511004, 0.017830834, -0.08593257, -0.0343...  \n",
       "5  [0.07102389, 0.029018747, -0.04301559, -0.0764...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.loc[:5, ['comment_text', 'encoded_comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "How do these features compare on discrimaniting between toxic / non-toxic comments? Let's put them in a raceoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "train_sizes = np.arange(0.1, 1.1, 0.1)\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "# estimator = GaussianNB()\n",
    "# estimator = RandomForestClassifier()\n",
    "\n",
    "def featurize(df=toxic_df):\n",
    "\n",
    "    labels = np.concatenate(lb.fit_transform(df.is_attack.values))\n",
    "    glove_features = np.vstack(df.glove_aggregate.values)\n",
    "    use_features = np.vstack(df.encoded_comment.values)\n",
    "\n",
    "    return labels, glove_features, use_features\n",
    "\n",
    "labels, glove_features, use_features = featurize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves:\n",
    "\n",
    "Learning curves allows us to visualize the performance of the system as a function of the amount of examples it's seen. We first create learning curves using the `glove_features`, and then we create learning curves of the `encoded_features`. We plot them together so we can compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 26.1 ms, total: 17.9 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "g_train_sizes, g_train_scores, g_test_scores = learning_curve(estimator=estimator, \n",
    "                                                              X=glove_features,\n",
    "                                                              y=labels, \n",
    "                                                              scoring=make_scorer(roc_auc_score),\n",
    "                                                              n_jobs=NUM_WORKERS, train_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 s, sys: 0 ns, total: 29.6 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "e_train_sizes, e_train_scores, e_test_scores = learning_curve(estimator=estimator, \n",
    "                                                              X=use_features,\n",
    "                                                              y=labels, \n",
    "                                                              scoring=make_scorer(roc_auc_score),\n",
    "                                                              n_jobs=NUM_WORKERS, train_sizes=train_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We used AUC as our scoring criteria, but you could also use accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5e30f19ac8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "results_df = pd.DataFrame({\"train_perc\": train_sizes,\n",
    "                           \"bow_auc\": np.mean(g_test_scores, axis=1),\n",
    "                           \"encoder_auc\": np.mean(e_test_scores, axis=1)})\n",
    "\n",
    "sns.lineplot(x=\"train_perc\", y=\"AUC\", hue=\"features\", \n",
    "             data=results_df.melt(\"train_perc\", var_name=\"features\", value_name=\"AUC\"), \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder features outperform the bag-of-words (BoW) glove vectors at every level of training data experience. While a more careful aggregation procedure of the word vectors would have done much better (in fact, there's good evidence that a thoughtful weighted-average can be [very hard to beat](https://openreview.net/forum?id=SyK00v5xx) on many discriminative tasks), the main point of this analysis is that using pre-trained encoders can basically be a drop-in replacement for word vectors for many applications and give significant gains, _modulo_ additional computation time to featurize the dataset (the BoW approach uses a lookup to compute features, which is very fast, whereas the encoder approach requires a full forward pass through a complicated recurrent neural network, which are inherently sequential (this is why there is a greater push towards [feed-forward architectures for language modeling](https://blog.openai.com/language-unsupervised/), which can be much faster during training and evaluation time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis __WARNING: FOUL LANGUAGE AHEAD__:\n",
    "\n",
    "Let's see some examples of where our model using language model features outperformed our BoW model. We'll be a bit more fair to both models this time and do a quick grid search to find a well-optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/embeddings/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/anaconda/envs/embeddings/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 s, sys: 1.22 s, total: 29.2 s\n",
      "Wall time: 28.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/embeddings/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_df = toxic_df.loc[:,['is_attack', 'comment_text', 'glove_aggregate', 'encoded_comment']]\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "train_df, test_df = train_test_split(model_df, train_size=0.75, random_state=12)\n",
    "\n",
    "def cv_predict_eval():\n",
    "\n",
    "#     cv = GridSearchCV(RandomForestClassifier(),\n",
    "#                      param_grid={\n",
    "#                          'n_estimators': [10, 100],\n",
    "#                          'max_features': ['sqrt', 'log2'],\n",
    "#                          'max_depth': [3, 5, None]\n",
    "#                      }, \n",
    "#                       refit=True, \n",
    "#                       n_jobs=NUM_WORKERS)\n",
    "\n",
    "    cv = LogisticRegression()\n",
    "\n",
    "    labels, glove_features, use_features = featurize(train_df)\n",
    "    labels_test, glove_test, use_test = featurize(test_df)\n",
    "\n",
    "    glove_fit = cv.fit(glove_features, labels)\n",
    "    glove_hat = glove_fit.predict(glove_test)\n",
    "    use_fit = cv.fit(use_features, labels)\n",
    "    use_hat = use_fit.predict(use_test)\n",
    "\n",
    "    results_df = test_df\n",
    "    results_df['use_pred'] = use_hat\n",
    "    results_df['glove_pred'] = glove_hat\n",
    "\n",
    "    return results_df\n",
    "\n",
    "results_df = cv_predict_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where BoW-GloVe Fails and the Encoder Succeeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>is_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102656</th>\n",
       "      <td>Be advised  I ve publicly responded to your...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105512</th>\n",
       "      <td>thanks a lot man, you dont know how long thats...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68289</th>\n",
       "      <td>OH MY IT S SO BIG AND BLACK</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77299</th>\n",
       "      <td>im half crazy figure out which side...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87038</th>\n",
       "      <td>stop with the warnings ill just be back lol...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89258</th>\n",
       "      <td>CAN I PLEASE BE UNBLOCKED I NEED TO MAKE A ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75616</th>\n",
       "      <td>I REALLY DONT GIVE TWO ABOUT BEING BANNED  ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69583</th>\n",
       "      <td>NO I HATE IT BECAUSE YOU DELETE EVERYTHING</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24483</th>\n",
       "      <td>Edit to Benito Ju rez</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74347</th>\n",
       "      <td>Hey, you ve got replies over there.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45435</th>\n",
       "      <td>LOL. The clique is afraid very afraid.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52624</th>\n",
       "      <td>BIGGER WINNER THAN YOU....</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60535</th>\n",
       "      <td>ha ha ha . ha ha ha ha ha . ha ha ha haha a h...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44206</th>\n",
       "      <td>mamas boy mamas boy mamas boy mamas boymamas b...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103405</th>\n",
       "      <td>Did you walk around for the entire event with ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>Eh Girlvinyl man Had me fooled.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56585</th>\n",
       "      <td>Joel Osteen Lakewood Church</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88354</th>\n",
       "      <td>calm your self boy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98728</th>\n",
       "      <td>san jose no clasificado</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5246</th>\n",
       "      <td>non mammal and highly carnivorous.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98134</th>\n",
       "      <td>Just plain Bill Just plain Bill</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108311</th>\n",
       "      <td>gonna do about it  Yeah ee yeah, yeah ee  Shak...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43562</th>\n",
       "      <td>Exactly these are Greeks Slavs ,Turks, Jewis...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73311</th>\n",
       "      <td>You could do worse.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50225</th>\n",
       "      <td>hello clown  Rust never sleeps</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69877</th>\n",
       "      <td>Sure, but then you stalked me</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91769</th>\n",
       "      <td>Wow. A snide rude response from Xeworlebi. The...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57827</th>\n",
       "      <td>jews  the only people that do conspiring is...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47355</th>\n",
       "      <td>hey wats up people</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18736</th>\n",
       "      <td>Strong disagree Just because a poodle is a d...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110585</th>\n",
       "      <td>EdJohnston Done</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>This is unhelpful. You wish a unilateral ri...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68224</th>\n",
       "      <td>Yup..That sums up..</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99678</th>\n",
       "      <td>THIS BLOCK IS COMPLETELY OUTSIDE OF POLICY I ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104914</th>\n",
       "      <td>Nice going   Guess you figured you didn t h...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68817</th>\n",
       "      <td>Another administator impersonator.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54537</th>\n",
       "      <td>Smile        Smile    Smile    Smile    Smile...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35430</th>\n",
       "      <td>why do you keep reverting my edits</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28687</th>\n",
       "      <td>you do know that you are a dupe, right just ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40243</th>\n",
       "      <td>. AND NO I HAVE NEVER USED AND OR CREATED ANOT...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40819</th>\n",
       "      <td>few bars of Molly Malone</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>HAve I got a hacker or something</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27904</th>\n",
       "      <td>JUST SMOEK WEEED TWICE AS HARD</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68390</th>\n",
       "      <td>so stop reverting the damn deletion</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>L2 SPELL RESPONSE SON BILLY MAYES OUT AGAIN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21908</th>\n",
       "      <td>. On second thought, don t</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105697</th>\n",
       "      <td>A barnstar for you  You get your Content Re...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50092</th>\n",
       "      <td>Smiles</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76735</th>\n",
       "      <td>Haven t you read WP BRD</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77442</th>\n",
       "      <td>why you cleen my mesage maybe i help you. gi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>ANYTHING YOU SAY CAN AND WILL BE USED AGAINST...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80028</th>\n",
       "      <td>CAPITAL PUNISHMENT   JUST LIKE BOLTE, USER ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41037</th>\n",
       "      <td>Reporting facts is a personal attack Surely ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92474</th>\n",
       "      <td>See That s what I mean D</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29758</th>\n",
       "      <td>DAVID BECKHAM IS BUFF</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97642</th>\n",
       "      <td>and Germans redirets to German people</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107210</th>\n",
       "      <td>Junge Dame beim Anlegen einer Perlenkette .....</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88181</th>\n",
       "      <td>UPDATED WITH REFERENCES NOW</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42905</th>\n",
       "      <td>I AM SOORY WE R NOT GIVING LEGAL THREATS TO ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16627</th>\n",
       "      <td>for a lying phony, Tijuana Brass</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  is_attack\n",
       "102656     Be advised  I ve publicly responded to your...      False\n",
       "105512  thanks a lot man, you dont know how long thats...      False\n",
       "68289                         OH MY IT S SO BIG AND BLACK      False\n",
       "77299              im half crazy figure out which side...      False\n",
       "87038      stop with the warnings ill just be back lol...      False\n",
       "89258      CAN I PLEASE BE UNBLOCKED I NEED TO MAKE A ...      False\n",
       "75616      I REALLY DONT GIVE TWO ABOUT BEING BANNED  ...      False\n",
       "69583          NO I HATE IT BECAUSE YOU DELETE EVERYTHING      False\n",
       "24483                            Edit to Benito Ju rez         False\n",
       "74347                Hey, you ve got replies over there.       False\n",
       "45435              LOL. The clique is afraid very afraid.      False\n",
       "52624                          BIGGER WINNER THAN YOU....      False\n",
       "60535    ha ha ha . ha ha ha ha ha . ha ha ha haha a h...      False\n",
       "44206   mamas boy mamas boy mamas boy mamas boymamas b...      False\n",
       "103405  Did you walk around for the entire event with ...      False\n",
       "3167                      Eh Girlvinyl man Had me fooled.      False\n",
       "56585                        Joel Osteen Lakewood Church       False\n",
       "88354                                  calm your self boy      False\n",
       "98728                             san jose no clasificado      False\n",
       "5246                   non mammal and highly carnivorous.      False\n",
       "98134                     Just plain Bill Just plain Bill      False\n",
       "108311  gonna do about it  Yeah ee yeah, yeah ee  Shak...      False\n",
       "43562     Exactly these are Greeks Slavs ,Turks, Jewis...      False\n",
       "73311                                 You could do worse.      False\n",
       "50225                      hello clown  Rust never sleeps      False\n",
       "69877                      Sure, but then you stalked me       False\n",
       "91769   Wow. A snide rude response from Xeworlebi. The...      False\n",
       "57827      jews  the only people that do conspiring is...      False\n",
       "47355                              hey wats up people          False\n",
       "18736     Strong disagree Just because a poodle is a d...      False\n",
       "...                                                   ...        ...\n",
       "110585                                   EdJohnston Done       False\n",
       "1553       This is unhelpful. You wish a unilateral ri...      False\n",
       "68224                                Yup..That sums up..       False\n",
       "99678    THIS BLOCK IS COMPLETELY OUTSIDE OF POLICY I ...      False\n",
       "104914     Nice going   Guess you figured you didn t h...      False\n",
       "68817                Another administator impersonator.        False\n",
       "54537    Smile        Smile    Smile    Smile    Smile...      False\n",
       "35430                why do you keep reverting my edits        False\n",
       "28687     you do know that you are a dupe, right just ...      False\n",
       "40243   . AND NO I HAVE NEVER USED AND OR CREATED ANOT...      False\n",
       "40819                           few bars of Molly Malone       False\n",
       "1667                    HAve I got a hacker or something       False\n",
       "27904                     JUST SMOEK WEEED TWICE AS HARD       False\n",
       "68390               so stop reverting the damn deletion        False\n",
       "44897        L2 SPELL RESPONSE SON BILLY MAYES OUT AGAIN       False\n",
       "21908                         . On second thought, don t       False\n",
       "105697     A barnstar for you  You get your Content Re...      False\n",
       "50092                                             Smiles       False\n",
       "76735                            Haven t you read WP BRD       False\n",
       "77442     why you cleen my mesage maybe i help you. gi...      False\n",
       "3032     ANYTHING YOU SAY CAN AND WILL BE USED AGAINST...      False\n",
       "80028      CAPITAL PUNISHMENT   JUST LIKE BOLTE, USER ...      False\n",
       "41037     Reporting facts is a personal attack Surely ...      False\n",
       "92474                           See That s what I mean D       False\n",
       "29758                              DAVID BECKHAM IS BUFF       False\n",
       "97642               and Germans redirets to German people      False\n",
       "107210    Junge Dame beim Anlegen einer Perlenkette .....      False\n",
       "88181                        UPDATED WITH REFERENCES NOW       False\n",
       "42905     I AM SOORY WE R NOT GIVING LEGAL THREATS TO ...      False\n",
       "16627                 for a lying phony, Tijuana Brass         False\n",
       "\n",
       "[469 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[(results_df[\"is_attack\"] == results_df[\"use_pred\"]) & \n",
    "               (results_df[\"is_attack\"] != results_df[\"glove_pred\"]) & \n",
    "               (results_df[\"is_attack\"] == False), \n",
    "               [\"comment_text\", \"is_attack\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i am sorry for vandalising i promise not to do it anymore'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[[59456], \"comment_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['thanks a lot man, you dont know how long thats been fuckin with my head  '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[[105512], \"comment_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's unfortunately pretty challenging to find any civil discussion on online forums. In the example highlighted above our BoW approach predicted the comment to be an attack, whereas the encoder correctly predicted it as benign. The prevalence of terms such \"dumb\", \"sorry\" _bias_ the average word vector to a representation that is more likely to be an attack than a nice comment, whereas the language model is able to encode the sequence of representations more accurately. A more acute example of this phenomena arises with swear words (ommitted here, but see for example record index `[105512]` for an example where the single occurrence of a curse word causes the BoW classifier to misclassify the sentiment dramatically (but the encoder gets it right). This also arises frequently in the use of **negation** in sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
