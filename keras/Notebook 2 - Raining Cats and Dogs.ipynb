{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Convolutional Neural Networks(CNNs) to identify dogs and cats: a binary classification problem\n",
    "\n",
    "This notebook contains code samples that have been adapted from Francois Chollet's Deep Learning With Python.\n",
    "\n",
    "----\n",
    "In this notebook we are going to introduce how to build a Convolutional Neural Network using [convolutional](https://keras.io/layers/convolutional/) layers. We are going to introduce the following layers:\n",
    "* `Conv2D`\n",
    "* `MaxPooling`\n",
    "* `Flatten`\n",
    "* `Dropout` (You have seen a small introduction to this in the previous notebook.\n",
    "\n",
    "We are also going to introduce training using Keras generator APIs such as `fit_generator`. Using Keras's `ImageDataGenerator` we will use image augmentation to fight overfitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "In our example CNN model we will classify images as \"dogs\" or \"cats\", in a dataset containing 4000 pictures of cats and dogs (2000 cats, 2000 dogs). We will use 2000 pictures for training, 1000 for validation, and finally 1000 for testing.\n",
    "\n",
    "In this section, we will start by naively training a small convnet on our 2000 training samples, without any regularization, to set a baseline for what can be achieved. We will introduce two techniques to train a model on small datasets:\n",
    "* Data augmentation [improves accuracy from 71% to 82%]\n",
    "* Feature extraction with a pre-trained network [improves accuracy to ~93%]\n",
    "\n",
    "There is a third technique which can increase accuracy further. But we will not be covering this since it requires training on a GPU.\n",
    "* Fine tuning a pre-trained network [improves accuracy to 95%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "You should have downloaded the cats vs dogs dataset from Kaggle as part of the Workshop setup instructions. If you haven't you can do so at this link: \n",
    "`https://www.kaggle.com/c/dogs-vs-cats/data` (you will need to create a Kaggle account if you don't already have one).\n",
    "\n",
    "The pictures are medium-resolution color JPEGs. They look like this:\n",
    "\n",
    "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). After downloading \n",
    "and uncompressing it, we will create a new dataset containing three subsets: a training set with 1000 samples of each class, a validation \n",
    "set with 500 samples of each class, and finally a test set with 500 samples of each class.\n",
    "\n",
    "Here are a few lines of code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = '/media/mike/HDD/data/cats_and_dogs/train'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = '/media/mike/HDD/data/cats_and_dogs/small/'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with our testing cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with our testing dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert 1000 == len(os.listdir(train_cats_dir))\n",
    "assert 1000 == len(os.listdir(train_dogs_dir))\n",
    "\n",
    "assert 500 == len(os.listdir(validation_cats_dir))\n",
    "assert 500 == len(os.listdir(validation_dogs_dir))\n",
    "\n",
    "assert 500 == len(os.listdir(test_cats_dir))\n",
    "assert 500 == len(os.listdir(test_dogs_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So we have indeed 2000 training images, and then 1000 validation images and 1000 test images. In each split, there is the same number of \n",
    "samples from each class: this is a balanced binary classification problem, which means that classification accuracy will be an appropriate \n",
    "measure of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "The general structure of our convnet will be a stack of alternated `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
    "\n",
    "However, since we are dealing with bigger images and a more complex problem, we will make our network accordingly: it will have 4 `Conv2D` + `MaxPooling2D` stages. \n",
    "\n",
    "A `Conv2D` layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. You can add a bias vector and activation to the output if needed. A `MaxPooling2D` layer is used to downscale input in\n",
    "both the vertical and horizontal dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Build your model with Conv2D and MaxPooling2D layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "# Instantiate a Sequential model\n",
    "model = models.Sequential() \n",
    "# Add a Conv2D layer with 32 filters, kernel_size of (3, 3) and `relu` activation. \n",
    "# Given that this is the first layer you need to specify the input shape of the input \n",
    "# without the sample size axis. For example: (128, 128, 3) represents\n",
    "# an input image of shape 128 X 128 RGB images and a data_format of \"channels_last\".\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "# Add a MaxPooling2D layer with a pool size of (2, 2).\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# Add a Conv2D layer with 64 filters, kernel_size of (3, 3) and `relu` activation.\n",
    "# ...\n",
    "# Add a MaxPooling2D layer with a pool size of (2, 2).\n",
    "# ...\n",
    "# Add a Conv2D layer with 128 filters, kernel_size of (3, 3) and `relu` activation.\n",
    "# ...\n",
    "# Add a MaxPooling2D layer with a pool size of (2, 2).\n",
    "# ...\n",
    "# Add a Conv2D layer with 128 filters, kernel_size of (3, 3) and `relu` activation.\n",
    "# ...\n",
    "# Add a MaxPooling2D layer with a pool size of (2, 2).\n",
    "# ...\n",
    "# Exercise 1: Solution\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Add a Flatten layer followed by 2 Dense layers:\n",
    "The next step would be to feed our last output tensor into a densely-connected classifier network like those you are already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. So first, we will have to flatten our 3D outputs to 1D using the `Flatten` layer and then add a few Dense layers on top to classify the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "# Use our model with Conv2D and MaxPooling2D layers from the previous section\n",
    "# ...\n",
    "# Add a Flatten layer\n",
    "# ...\n",
    "# Add a Dense layer of 512 units and `relu` activation.\n",
    "# ...\n",
    "# Add a Dense layer of 1 unit and `sigmoid` activation.\n",
    "\n",
    "# Exercise 2: Solution\n",
    "# Our model from the previous section\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# The newly added Flatten and Dense layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the dimensions of the feature maps change with every successive layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the depth of the feature maps is progressively increasing in the network (from 32 to 128), while the size of the feature maps is \n",
    "decreasing (from 148x148 to 7x7). This is a pattern that you will see in almost all convnets.\n",
    "\n",
    "Since we are attacking a binary classification problem, we are ending the network with a single unit (a `Dense` layer of size 1) and a \n",
    "`sigmoid` activation. This unit will encode the probability that the network is looking at one class or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# `compile` the model using `binary_crossentropy` loss, `RMSProp` optimizer and \n",
    "# `accuracy` metric.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataGenerator\n",
    "\n",
    "As you already know by now, data should be formatted into appropriately pre-processed floating point tensors before being fed into our \n",
    "network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:\n",
    "\n",
    "* Read the picture files.\n",
    "* Decode the JPEG content to RBG grids of pixels.\n",
    "* Convert these into floating point tensors.\n",
    "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values).\n",
    "\n",
    "Keras has utilities to take care of these steps automatically. Keras has a module with image \n",
    "processing helper tools, located at `keras.preprocessing.image`. In particular, it contains the class [`ImageDataGenerator`](https://keras.io/preprocessing/image/) which allows to quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we will use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output of one of these generators: it yields batches of 150x150 RGB images (shape `(20, 150, 150, 3)`) and binary \n",
    "labels (shape `(20,)`). 20 is the number of samples in each batch (the batch size). Note that the generator yields these batches \n",
    "indefinitely: it just loops endlessly over the images present in the target folder. For this reason, we need to `break` the iteration loop \n",
    "at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our model to the data using the generator.We do it using the `fit_generator` method, the equivalent of `fit` for data generators like ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Train the model using the `fit_generator` method:\n",
    "\n",
    "We need the `steps_per_epoch` and `validation_steps` argument to specify the batches drawn from the generator since the generator yields data endlessly.\n",
    "Note: To run through the entire input dataset, the `steps_per_epoch` argument should be num_inputs_samples/generator_batch_size = 2000/20 = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 105s 2s/step - loss: 0.6928 - acc: 0.5110 - val_loss: 0.6803 - val_acc: 0.5480\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 75s 1s/step - loss: 0.6807 - acc: 0.5640 - val_loss: 0.6592 - val_acc: 0.6230\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 71s 1s/step - loss: 0.6576 - acc: 0.6240 - val_loss: 0.6501 - val_acc: 0.6150\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 73s 1s/step - loss: 0.6408 - acc: 0.6390 - val_loss: 0.6524 - val_acc: 0.5790\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 79s 2s/step - loss: 0.6191 - acc: 0.6530 - val_loss: 0.6149 - val_acc: 0.6730\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 70s 1s/step - loss: 0.5975 - acc: 0.6730 - val_loss: 0.6104 - val_acc: 0.6620\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 74s 1s/step - loss: 0.5656 - acc: 0.6920 - val_loss: 0.6386 - val_acc: 0.6420\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 80s 2s/step - loss: 0.5710 - acc: 0.6990 - val_loss: 0.6036 - val_acc: 0.6610\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 73s 1s/step - loss: 0.5453 - acc: 0.7380 - val_loss: 0.5872 - val_acc: 0.6900\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 70s 1s/step - loss: 0.5301 - acc: 0.7250 - val_loss: 0.5938 - val_acc: 0.6730\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 67s 1s/step - loss: 0.5112 - acc: 0.7450 - val_loss: 0.5901 - val_acc: 0.6840\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 70s 1s/step - loss: 0.5080 - acc: 0.7490 - val_loss: 0.6106 - val_acc: 0.6710\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 60s 1s/step - loss: 0.4925 - acc: 0.7620 - val_loss: 0.5873 - val_acc: 0.7000\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 62s 1s/step - loss: 0.4639 - acc: 0.7880 - val_loss: 0.5732 - val_acc: 0.7150\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 65s 1s/step - loss: 0.4779 - acc: 0.7710 - val_loss: 0.5653 - val_acc: 0.7080\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.4435 - acc: 0.7950 - val_loss: 0.5712 - val_acc: 0.7100\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 61s 1s/step - loss: 0.4103 - acc: 0.8010 - val_loss: 0.5814 - val_acc: 0.7000\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 62s 1s/step - loss: 0.4510 - acc: 0.7960 - val_loss: 0.5679 - val_acc: 0.7050\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 59s 1s/step - loss: 0.4194 - acc: 0.8070 - val_loss: 0.5673 - val_acc: 0.7180\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 63s 1s/step - loss: 0.3937 - acc: 0.8320 - val_loss: 0.5820 - val_acc: 0.7010\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 62s 1s/step - loss: 0.3770 - acc: 0.8330 - val_loss: 0.5650 - val_acc: 0.7150\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 60s 1s/step - loss: 0.4056 - acc: 0.8200 - val_loss: 0.5699 - val_acc: 0.7140\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 63s 1s/step - loss: 0.3541 - acc: 0.8490 - val_loss: 0.6608 - val_acc: 0.6970\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 58s 1s/step - loss: 0.3672 - acc: 0.8420 - val_loss: 0.5842 - val_acc: 0.7160\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 60s 1s/step - loss: 0.3343 - acc: 0.8550 - val_loss: 0.5887 - val_acc: 0.7210\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 60s 1s/step - loss: 0.3395 - acc: 0.8590 - val_loss: 0.6127 - val_acc: 0.7040\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 67s 1s/step - loss: 0.2941 - acc: 0.8720 - val_loss: 0.6076 - val_acc: 0.7120\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 67s 1s/step - loss: 0.3322 - acc: 0.8610 - val_loss: 0.6140 - val_acc: 0.7070\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 65s 1s/step - loss: 0.2805 - acc: 0.8860 - val_loss: 0.6334 - val_acc: 0.7120\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 64s 1s/step - loss: 0.2892 - acc: 0.8820 - val_loss: 0.7434 - val_acc: 0.6980\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the `fit_generator` method with the `train_generator` created \n",
    "# above as the first argument.\n",
    "# The other parameters to this API are similar to the `fit` function.\n",
    "# `steps_per_epoch` to specify the number of batches to draw from the generator.\n",
    "# `epochs` that we want to train the model for.\n",
    "# `validation_data` argument which will be the validation generator\n",
    "# `validation_steps` to specify the number of batches to draw from the generator.\n",
    "# history = model.fit_generator(...)\n",
    "\n",
    "# Exercise 3: Solution\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=50,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is good practice to always save your models after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras also provides APIs to load your model from an existing h5 file using `load`. You can also use the `save_weights` and `load_weights` APIs to only save/restore layer weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the loss and accuracy of the model over the training and validation data during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that training accuracy increases linearly to reach almost 100%. Training loss should also decrease linearly until it reaches nearly 0.\n",
    "Verify that validation accuracy stalls at 70-72% and validation loss reaches its minima after only 5 epochs and stalls.\n",
    "These plots are characteristic of overfitting. \n",
    "\n",
    "We are now going to introduce the first technique to deal with overfitting, specific to computer vision, and used almost universally when processing images with deep learning models: *data augmentation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Data Augmentation\n",
    "\n",
    "Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. Data augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same picture twice. This helps the model get exposed to more aspects of the data and generalize better.\n",
    "\n",
    "In Keras, this can be done by configuring a number of random transformations to be performed on the images read by our `ImageDataGenerator` instance. Let's get started with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just a few of the options available (for more, see the Keras documentation). Let's quickly go over what we just wrote:\n",
    "\n",
    "* `rotation_range` is a value in degrees (0-180), a range within which to randomly rotate pictures.\n",
    "* `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures \n",
    "vertically or horizontally.\n",
    "* `shear_range` is for randomly applying shearing transformations.\n",
    "* `zoom_range` is for randomly zooming inside pictures.\n",
    "* `horizontal_flip` is for randomly flipping half of the images horizontally -- relevant when there are no assumptions of horizontal \n",
    "asymmetry (e.g. real-world pictures).\n",
    "* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
    "\n",
    "Let's take a look at our augmented images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is module with image preprocessing utilities\n",
    "from keras.preprocessing import image\n",
    "\n",
    "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
    "\n",
    "# We pick one image to \"augment\"\n",
    "img_path = fnames[3]\n",
    "\n",
    "# Read the image and resize it\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "\n",
    "# Convert it to a Numpy array with shape (150, 150, 3)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Reshape it to (1, 150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# The .flow() command below generates batches of randomly transformed images.\n",
    "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we train a new network using this data augmentation configuration, our network will never see twice the same input. However, the inputs \n",
    "that it sees are still heavily intercorrelated, since they come from a small number of original images -- we cannot produce new information, \n",
    "we can only remix existing information. As such, this might not be quite enough to completely get rid of overfitting. To further fight \n",
    "overfitting, we will also add a Dropout layer to our model, right before the densely-connected classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Add a Dropout layer right before the fully connected Dense layers with a rate value of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "# Add the Dropout layer with a dropout rate of 0.5\n",
    "# ...\n",
    "model.add(layers.Dropout(0.5))\n",
    "# FCC Dense layers\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile your model using a `binary_crossentropy` loss, `RMPProp` \n",
    "# optimizer(learning rateis 1e-4) and `accuracy` as one of the metrics.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train our network using data augmentation and dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "20/20 [==============================] - 49s 2s/step - loss: 0.6968 - acc: 0.5078 - val_loss: 0.6934 - val_acc: 0.4981\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.6905 - acc: 0.5279 - val_loss: 0.6891 - val_acc: 0.5019\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 49s 2s/step - loss: 0.6873 - acc: 0.5375 - val_loss: 0.6894 - val_acc: 0.4911\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 51s 3s/step - loss: 0.6893 - acc: 0.5234 - val_loss: 0.6792 - val_acc: 0.5247\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 59s 3s/step - loss: 0.6854 - acc: 0.5314 - val_loss: 0.6759 - val_acc: 0.5831\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 55s 3s/step - loss: 0.6861 - acc: 0.5422 - val_loss: 0.6968 - val_acc: 0.5019\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 58s 3s/step - loss: 0.6804 - acc: 0.5686 - val_loss: 0.6869 - val_acc: 0.5102\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.6853 - acc: 0.5359 - val_loss: 0.6712 - val_acc: 0.5463\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 52s 3s/step - loss: 0.6673 - acc: 0.5828 - val_loss: 0.6608 - val_acc: 0.5831\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 50s 3s/step - loss: 0.6610 - acc: 0.6000 - val_loss: 0.6753 - val_acc: 0.5692\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 51s 3s/step - loss: 0.6722 - acc: 0.5872 - val_loss: 0.6585 - val_acc: 0.5825\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 0.6708 - acc: 0.5813 - val_loss: 0.6570 - val_acc: 0.6047\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.6698 - acc: 0.5906 - val_loss: 0.6516 - val_acc: 0.6034\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 61s 3s/step - loss: 0.6802 - acc: 0.5691 - val_loss: 0.6512 - val_acc: 0.6053\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 60s 3s/step - loss: 0.6632 - acc: 0.5766 - val_loss: 0.6934 - val_acc: 0.5127\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 50s 3s/step - loss: 0.6506 - acc: 0.6312 - val_loss: 0.6407 - val_acc: 0.6117\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 51s 3s/step - loss: 0.6751 - acc: 0.5906 - val_loss: 0.6626 - val_acc: 0.5787\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 54s 3s/step - loss: 0.6520 - acc: 0.6157 - val_loss: 0.6468 - val_acc: 0.6066\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 47s 2s/step - loss: 0.6535 - acc: 0.6062 - val_loss: 0.6837 - val_acc: 0.5679\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 51s 3s/step - loss: 0.6475 - acc: 0.6234 - val_loss: 0.6354 - val_acc: 0.6218\n"
     ]
    }
   ],
   "source": [
    "# Define our train ImageDataGenerator from above\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Define our test ImageDataGenerator\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define parameters for batch generation from train data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Define parameters for batch generation from test data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "# Train the model using `fit_generator`\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=20,\n",
    "      epochs=20,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "# Note you can train for steps_per_epoch=100 and epochs=100 to get an accuracy of 82%.\n",
    "# However in the interest of time you can reduce the steps_per_epoch and number of epochs to\n",
    "# 50 and 20 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our results again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhIm+YRhGDcJE3zAMowZhom8YhlGD+P8v+mUrE7DH6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fecb80beef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec920792b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the training and validation curves are closely tracking each other.\n",
    "Verify that we can now reach an accuracy of 72% for 20 epochs with 50 steps per epoch.\n",
    "\n",
    "Thanks to data augmentation and dropout, we are no longer overfitting: the training curves are rather closely tracking the validation \n",
    "curves. We are now able to reach an accuracy of ~72% if we train for 20 epochs. If we train for a 100 epochs we can achieve an accuracy of 82%, a 15% relative improvement over the non-regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Exercise(optional): Use L2 regularization in our layers to see if we can achieve even better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "The second technique that we will use to train an image classification model on a small dataset is Feature Extraction using a pre-trained network. A pre-trained network is simply a saved network previously trained on a large dataset, typically on a large-scale image classification task. If this original dataset is large enough and general enough, then the spatial feature hierarchy learned by the pre-trained network can effectively act as a generic model of our visual world, and hence its features can prove useful for many different computer vision problems, even though these new problems might involve completely different classes from those of the original task.\n",
    "\n",
    "We will use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman in 2014, a simple and widely used convnet architecture for ImageNet dataset(1.4 million labeled images and 1000 different classes). Keras comes prepackaged with a number of [image classification models](https://keras.io/applications/). VGG16 is one such model that is available as part of `keras.applications`. All the models are pre-trained on the ImageNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. \n",
    "These features are then run through a new classifier, which is trained from scratch.\n",
    "\n",
    "![swapping FC classifiers](https://s3.amazonaws.com/book.keras.io/img/ch5/swapping_fc_classifier.png)\n",
    "\n",
    "Note that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on \n",
    "the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual \n",
    "edges, colors, and textures), while layers higher-up extract more abstract concepts (such as \"cat ear\" or \"dog eye\"). So if your new \n",
    "dataset differs a lot from the dataset that the original model was trained on, you may be better off using only the first few layers of the \n",
    "model to do feature extraction, rather than using the entire convolutional base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the VGG16 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 555s 9us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We passed three arguments to the constructor:\n",
    "\n",
    "* `weights`, to specify which weight checkpoint to initialize the model from\n",
    "* `include_top`, which refers to including or not the densely-connected classifier on top of the network. By default, this \n",
    "densely-connected classifier would correspond to the 1000 classes from ImageNet. Since we intend to use our own densely-connected \n",
    "classifier (with only two classes, cat and dog), we don't need to include it.\n",
    "* `input_shape`, the shape of the image tensors that we will feed to the network. This argument is purely optional: if we don't pass it, \n",
    "then the network will be able to process inputs of any size.\n",
    "\n",
    "Here's the detail of the architecture of the VGG16 convolutional base: it's very similar to the simple convnets that you are already \n",
    "familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature map has shape `(4, 4, 512)`. That's the feature on top of which we will stick a densely-connected classifier.\n",
    "\n",
    "At this point, there are two ways we could proceed: \n",
    "\n",
    "* Running the convolutional base over our dataset, recording its output to a Numpy array on disk, then using this data as input to a standalone densely-connected classifier. This involves running the convolutional base once for every input image. However, for the exact same reason, this technique would not allow us to leverage data augmentation at \n",
    "all.\n",
    "* Extending the model we have (`conv_base`) by adding `Dense` layers on top, and running the whole thing end-to-end on the input data. This allows us to use data augmentation, because every input image is going through the convolutional base every time it is seen by the model. However, for this same reason, this technique is far more expensive than the first one.\n",
    "\n",
    "Let's walk through the code required to set-up the first one: recording the output of `conv_base` on our \n",
    "data and using these outputs as inputs to a new model.\n",
    "\n",
    "We will start by simply running instances of the previously-introduced `ImageDataGenerator` to extract images as Numpy arrays as well as their labels. We will extract features from these images simply by calling the `predict` method of the `conv_base` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "base_dir = '/Users/anjalisridhar/Downloads/cats_and_dogs_small'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted features are currently of shape `(samples, 4, 4, 512)`. We will feed them to a densely-connected classifier, so first we must \n",
    "flatten them to `(samples, 8192)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can define our densely-connected classifier (note the use of dropout for regularization), and train it on the data and labels that we just recorded:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the trained features through a Dense classifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train the model using the extracted features and labels.\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is very fast, since we only have to deal with two `Dense` layers -- an epoch takes less than one second even on CPU. You should have reached a validation accuracy of ~90% in 30 epochs.\n",
    "\n",
    "Let's take a look at the loss and accuracy curves during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach a validation accuracy of about 90%, much better than what we could achieve in the previous section (~84%) with our small model trained from \n",
    "scratch. However, our plots also indicate that we are overfitting almost from the start -- despite using dropout with a fairly large rate. \n",
    "This is because this technique does not leverage data augmentation, which is essential to preventing overfitting with small image datasets.\n",
    "\n",
    "\n",
    "Another technique used to deal with small datasets is *Fine Tuning*. As mentioned in the beginning of the notebook, this technique is extremely slow and is practically impossible to run on CPU. You will need a GPU for this technique. You can create a `Sequential` model and add the `conv_base` as one of its layers. The output of the base is then flattened and run through a Dense fully connected network as we have seen before.\n",
    "\n",
    "The steps for fine-tuning a network are as follow:\n",
    "\n",
    "* 1) Add your custom network on top of an already trained base network.\n",
    "* 2) Freeze the base network.\n",
    "* 3) Train the part you added.\n",
    "* 4) Unfreeze some layers in the base network.\n",
    "* 5) Jointly train both these layers and the part you added.\n",
    "\n",
    "\n",
    "We have already completed the first 3 steps when doing feature extraction. To proceed with the 4th step we will need to unfreeze our `conv_base`, and then freeze individual layers inside of it. You can set the `trainable` property of a layer as needed. \n",
    "\n",
    "Note: We need to first train the dense classifier we added to the pre-trained network before unfreezing the convolutional base. This is to prevent a large error signal from propagating through the network during training. The representations learned by the network previously will be destroyed otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Solution\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 2: Solution\n",
    "# Our model from the previous section\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "#                         input_shape=(150, 150, 3)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# The newly added Flatten and Dense layers\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(512, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Solution\n",
    "# history = model.fit_generator(\n",
    "#       train_generator,\n",
    "#       steps_per_epoch=50,\n",
    "#       epochs=30,\n",
    "#       validation_data=validation_generator,\n",
    "#       validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 4: Solution\n",
    "# model.add(layers.Dropout(0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
